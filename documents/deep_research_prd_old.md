# 📄 Product Requirements Document (PRD)

## 💡 프로젝트명: 딥 리서치 챗봇 (Deep Research Chatbot)

---

## 1. 개요 (Overview)

deep research chatbot은 LangChain, LangGraph, LangSmith 기반의 멀티 에이전트 시스템으로, 사용자의 질문에 대해 다양한 관점에서 심층적으로 조사한 결과를 제공합니다. 검색엔진 병렬 활용과 문서 요약, 출처 기반 응답을 통해 신뢰도 높은 정보를 제공하며, Streamlit UI로 단순하고 명료하게 인터페이스를 구성합니다.

---

## 2. 사용자 정의 (Target User)

- 정보 탐색의 깊이를 원하는 **비전문 일반인 \~ 준연구자**
  - 예: 기획자, 대학생, 작가, 스타트업 창업자 등
- 배경지식은 있으나 논문 검색이나 전문 DB는 부담스러운 사용자층
- ChatGPT와 같은 대화형 인터페이스에 익숙한 사용자

---

## 3. 핵심 기능 (Key Features)

### 3.1 질문 확장 → 다중 쿼리 생성

- 사용자 입력(질문 또는 키워드)을 분석하여 하위 질문(sub-queries) 자동 생성
- LangChain 기반 Prompt Template 또는 LLM을 이용한 확장

### 3.2 병렬 웹 검색 (DuckDuckGo + Tavily)

- 생성된 서브 쿼리를 Tavily와 DuckDuckGo에서 병렬 검색
- 뉴스 및 블로그 기반 우선 수집, 논문 및 리포트는 선택적으로 활용
- 정확도 높은 문서 우선 정렬, 중복 제거

### 3.3 문서 요약 + 출처 포함 응답 생성

- 수집된 문서를 멀티 문서 요약
- 마크다운 형식으로 구조화된 응답 생성
- 각 문장 옆에 출처 인라인 표기

### 3.4 검증 및 반복 루프 (Validation Agent)

- 응답 생성 후 검증 에이전트가 논리적 오류, 부정확성 여부 확인
- 필요 시 일부 에이전트 재호출 (루프 1\~2회 제한)

### 3.5 진행 상태 출력

- 각 에이전트 단계별 상태를 Streamlit UI 상에 표시
  - 예: 질문 분석 완료 / 웹 검색 중 / 요약 중 / 검증 중 ...

---

## 4. 사용자 인터페이스 (UI)

- **Streamlit 기반 단일 창 구성**

  - 상단: 텍스트 입력창 (사용자가 질문 입력)
  - 중단: 진행 상태 표시 (에이전트 진행 상황 실시간 업데이트)
  - 하단: 구조화된 마크다운 응답 출력 (출처 포함)

- **UI 표시 항목 최소화**

  - 사용자가 작성한 질문
  - 시스템이 응답한 마크다운 정리본
  - 진행 상태 메시지 (예: "검색 중", "요약 중", "응답 생성 완료")

- **불필요한 기능 배제**

  - 세션 저장, 대화 기록 없음
  - 단일 입력 → 결과 확인 흐름만 유지

### 💡 Streamlit UI 시나리오 예시

1. 사용자가 질문 입력

   - 입력창: `오픈소스 LLM 트렌드 알려줘`

2. 중간 진행 표시

   - 메시지:
     ```
     [질문 분석 중...]
     [웹 검색 실행 중 (DuckDuckGo + Tavily)...]
     [문서 요약 중...]
     [최종 응답 생성 중...]
     ```

3. 결과 출력

   - 사용자의 질문: `오픈소스 LLM 트렌드 알려줘`
   - 마크다운 응답:
     ```
     ## 오픈소스 LLM 트렌드
     - 최근 공개된 Mistral 모델은 성능이 높고 경량화되었다. *(source: huggingface.co)*
     - Meta의 LLaMA2는 상업적 사용까지 허용되며 영향력이 크다. *(source: arxiv.org)*
     ```

- **Streamlit 기반 단일 창 구성**

  - 상단: 텍스트 입력창
  - 중단: 진행 상태 표시
  - 하단: 구조화된 마크다운 응답 출력

- **불필요한 기능 배제**

  - 세션 저장, 대화 기록 없음
  - 단일 입력 → 결과 확인 흐름만 유지

---

## 5. 멀티 에이전트 구성 (Agent Architecture)

### 💬 LLM 프롬프트 / 응답 구조 예시

아래는 각 에이전트가 사용하는 LLM 입력 프롬프트와 기대 응답 형식의 예시입니다. 실제 모델은 GPT 계열 또는 Gemini 계열 중 하나를 선택해 통일하여 사용합니다.

#### 🧾 질문 분석 에이전트

**프롬프트 템플릿:**

```
다음 사용자 질문을 기반으로 조사에 도움이 될 세부 질문을 3~5개 생성해줘:

질문: {user_input}
```

**예상 응답:**

```json
[
  "오픈소스 LLM의 최신 트렌드는 무엇인가요?",
  "Mistral이나 LLaMA와 같은 주요 오픈소스 모델은 어떤 특징이 있나요?"
]
```

#### 📚 문서 요약 에이전트

**프롬프트 템플릿:**

```
다음 기사 내용을 3줄 이내로 요약해줘:

문서: {content}
```

**예상 응답:**

```
이 문서는 GPT-4o가 공개된 이후 업계 반응을 다룬 뉴스로, 높은 성능과 공개 API로 인해 활용도가 높아지고 있다는 점을 강조한다.
```

#### 🧠 인사이트 에이전트

**프롬프트 템플릿:**

```
다음 요약된 문서들을 기반으로 핵심 인사이트 3가지를 도출해줘:

요약 리스트:
- {summary_1}
- {summary_2} ...
```

**예상 응답:**

```json
[
  "기업들은 오픈소스 LLM을 비용 절감과 유연성 때문에 채택하고 있다.",
  "상업용 모델과 성능 격차가 빠르게 좁혀지고 있다."
]
```

#### 🧩 응답 생성 에이전트

**프롬프트 템플릿:**

```
다음 인사이트와 문서 출처를 바탕으로 마크다운 형식의 응답을 생성해줘. 각 문장 옆에 출처를 명시해줘:

인사이트: {insights}
문서 리스트: {documents}
```

**예상 응답:**

```
## 오픈소스 LLM 트렌드

- Mistral과 LLaMA는 오픈소스 LLM 중에서도 성능이 높고 활용도가 크다. *(source: huggingface.co)*
- 기업들은 오픈소스 모델을 통해 비용을 절감하고 있다. *(source: forbes.com)*
```

### 🧾 상태(State) 구조 정의

LangGraph 전체 워크플로우에서 공유되는 상태 객체는 각 노드 간 데이터를 전달하는 매개체로 사용되며, 다음과 같은 필드를 포함합니다:

```python
State = {
  "user_input": str,                  # 사용자 입력 원문
  "sub_queries": List[str],           # 질문 분석 결과 생성된 하위 쿼리 목록
  "documents": List[Dict],            # 검색 결과로 수집된 문서 리스트 (title, url, content)
  "summaries": List[str],             # 각 문서 요약 결과
  "insights": List[str],              # 인사이트 도출 결과
  "markdown_answer": str,             # 마크다운 형식 응답 (출처 포함)
  "is_valid": bool,                   # 검증 결과 여부
  "retry_count": int,                 # 검증 실패 시 반복 횟수
  "logs": List[str]                   # 진행 상태 로깅 (에이전트별 로그 추적용)
}
```

- 모든 노드는 `state`를 입력받고 수정하여 반환
- LangChain `RunnableWithMessageHistory` 또는 LangGraph `StateGraph` 기반으로 활용 가능
- LangSmith에서 상태 필드 추적도 가능하도록 구조화

### 📥 LangGraph 노드별 입력/출력 구조

| 노드 이름                | 입력 데이터                | 출력 데이터                                          |
| -------------------- | --------------------- | ----------------------------------------------- |
| `input_node`         | 사용자 자연어 질문 (str)      | 질문 (str)                                        |
| `question_analyzer`  | 질문 (str)              | 하위 쿼리 리스트 (List[str])                           |
| `multi_search`       | 하위 쿼리 리스트 (List[str]) | 문서 리스트 (List[Dict], 각 문서에 title/url/content 포함) |
| `doc_summarizer`     | 문서 리스트                | 요약 리스트 (List[str])                              |
| `insight_generator`  | 요약 리스트                | 인사이트 리스트 (List[str])                            |
| `response_generator` | 인사이트 리스트, 문서 리스트      | 마크다운 응답 (str), 출처 포함 응답 (str)                   |
| `response_validator` | 마크다운 응답, 출처 포함 응답     | 검증 결과 (bool), 수정 요청 여부                          |
| `final_output`       | 응답 결과 (str)           | 사용자에게 출력될 최종 결과                                 |



- **노드 정의:**

  - `input_node` → 사용자 질문 입력 노드
  - `question_analyzer` → 질문 분석 에이전트 노드
  - `multi_search` → 병렬 검색 실행 노드
  - `doc_summarizer` → 문서 요약 노드
  - `insight_generator` → 인사이트 생성 노드
  - `response_generator` → 응답 생성 노드
  - `response_validator` → 응답 검증 노드
  - `final_output` → 결과 출력 노드

- **노드 연결 흐름:**

  ```
  input_node → question_analyzer → multi_search → doc_summarizer → insight_generator → response_generator → response_validator

  response_validator →
      ├── final_output (if valid)
      └── insight_generator 또는 response_generator (if invalid, loop max 2 times)
  ```

\----------|------| | 🧾 질문 분석 에이전트 | 사용자 입력을 분해하고 하위 쿼리 생성 | | 🌐 웹 검색 에이전트 | DuckDuckGo, Tavily 병렬 검색 실행 | | 📚 문서 요약 에이전트 | 수집된 문서 요약 | | 🧠 인사이트 에이전트 | 요약으로부터 시사점 추출 | | 🧩 응답 생성 에이전트 | 마크다운 응답 생성 + 출처 정리 | | ✅ 검증 에이전트 | 최종 응답 검토 및 필요시 반복 요청 |

- **제어 흐름:** LangGraph 기반
- **모니터링:** LangSmith 적용 (각 에이전트 호출, 응답, 시간 추적)

### 🔁 에이전트 간 데이터 흐름도 (텍스트 기반)

```
[사용자 입력]
     │
     ▼
[🧾 질문 분석 에이전트]
 - 입력된 질문을 분석하여 하위 쿼리(서브 질문)들을 생성
     │
     ▼
[🌐 웹 검색 에이전트]
 - DuckDuckGo + Tavily에서 병렬 검색 수행
 - 각 하위 쿼리에 대해 웹 검색 결과 수집
     │
     ▼
[📚 문서 요약 에이전트]
 - 수집된 문서를 각각 요약
 - 핵심 정보만 추출
     │
     ▼
[🧠 인사이트 에이전트]
 - 요약된 문서들로부터 인사이트(시사점, 핵심 논점) 도출
     │
     ▼
[🧩 응답 생성 에이전트]
 - 전체 내용을 마크다운 구조로 정리
 - 각 문장 옆에 출처 포함
     │
     ▼
[✅ 검증 에이전트]
 - 최종 응답을 검토 (논리 오류, 불일치 여부 등)
     ├── 문제가 없으면 → [최종 출력]
     └── 문제가 있으면 → 다시 [🧠 인사이트] 또는 [🧩 응답 생성]으로 되돌아가 반복
```

\----------|------| | 🧾 질문 분석 에이전트 | 사용자 입력을 분해하고 하위 쿼리 생성 | | 🌐 웹 검색 에이전트 | DuckDuckGo, Tavily 병렬 검색 실행 | | 📚 문서 요약 에이전트 | 수집된 문서 요약 | | 🧠 인사이트 에이전트 | 요약으로부터 시사점 추출 | | 🧩 응답 생성 에이전트 | 마크다운 응답 생성 + 출처 정리 | | ✅ 검증 에이전트 | 최종 응답 검토 및 필요시 반복 요청 |

- **제어 흐름:** LangGraph 기반
- **모니터링:** LangSmith 적용 (각 에이전트 호출, 응답, 시간 추적)

---

## 6. 기술 스택 (Tech Stack)

| 영역            | 기술                                                                                                                                                 |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| 멀티 에이전트 프레임워크 | LangChain + LangGraph                                                                                                                              |
| 검색 API        | DuckDuckGo, Tavily                                                                                                                                 |
| LLM 및 임베딩     | GPT 계열(GPT-4o, GPT-4o-mini, text-embedding-3-large) 또는 Gemini 계열(Gemini 2.5 Flash, Gemini 2.5 Pro, gemini-embedding-001) 중 하나로 통일하여 사용. 사용자가 선택 가능 |
| 벡터 DB         | Chroma                                                                                                                                             |
| 인터페이스         | Streamlit                                                                                                                                          |
| 환경 변수 관리      | `.env` 파일 기반 구성 (API 키, LLM 종류 등)                                                                                                                  |
| 실행 환경         | Python 가상환경 (venv 또는 poetry 등)에서 로컬 실행                                                                                                             |
| 모니터링          | LangSmith (설정만 적용, 사용자는 직접 확인)                                                                                                                     |

| 영역            | 기술                                                                                                                                     |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| 멀티 에이전트 프레임워크 | LangChain + LangGraph                                                                                                                  |
| 검색 API        | DuckDuckGo, Tavily                                                                                                                     |
| LLM 및 임베딩     | GPT 계열(GPT-4o, GPT-4o-mini, text-embedding-3-large) 또는 Gemini 계열(Gemini 2.5 Flash, Gemini 2.5 Pro, gemini-embedding-001) 중 하나로 통일하여 사용 |
| 벡터 DB         | Chroma                                                                                                                                 |
| 인터페이스         | Streamlit                                                                                                                              |
| 모니터링          | LangSmith                                                                                                                              |

---

## 7. MVP 범위 (Minimum Viable Product)

1. 질문 분석 → 하위 쿼리 자동 생성
2. 병렬 검색 → 뉴스 및 블로그 정보 수집
3. 문서 요약 → 마크다운 구조화 + 출처 포함
4. 검증 에이전트 1회 반복 루프
5. Streamlit UI 최소 구성 + 진행 상태 표시

---

## 🔧 개발 환경 메모

- `.env` 예시

  ```env
  LLM_PROVIDER=gpt
  OPENAI_API_KEY=sk-...
  GEMINI_API_KEY=...
  ```

- 실행 예시 (가상환경 기준)

  ```bash
  source venv/bin/activate
  streamlit run main.py
  ```

## 8. 평가 지표 및 테스트 시나리오

### 🎯 평가 지표 (Evaluation Metrics)

| 항목         | 설명                         | 기준                    |
| ---------- | -------------------------- | --------------------- |
| 응답 구조화 완성도 | 마크다운 응답이 명확한 구조로 작성되어 있는가  | 제목/소제목/글머리표 존재 여부로 체크 |
| 출처 명확성     | 각 문장 또는 인사이트에 출처가 잘 달려 있는가 | 출처 누락 문장 1개 이하        |
| 중복 제거 정확도  | 검색 결과에서 유사 문서가 제거되었는가      | 중복 문서 10% 이하          |
| 인사이트 품질    | 단순 요약이 아닌 새로운 관찰/시사점이 있는가  | 사람 평가 또는 예시 답안 비교     |
| 응답 타당성     | 잘못된 정보, 오류, 논리적 비약이 없는가    | 검증 에이전트 통과 여부         |

### 🧪 테스트 시나리오 (Test Scenarios)

1. **기본 사용 흐름**

   - 입력: "오픈소스 LLM 트렌드 알려줘"
   - 기대: 질문 확장 → 검색 → 요약 → 응답 생성 → 응답 출력까지 문제없이 완료

2. **짧은 키워드 입력**

   - 입력: "Gemini"
   - 기대: 질문 분석으로 세부 쿼리 생성됨 → 정상 응답 생성됨

3. **중복 검색 결과 처리**

   - 입력: "GPT-4o 성능"
   - 기대: 동일한 뉴스가 여러 검색 엔진에서 나올 경우, 하나만 요약에 반영됨

4. **응답 오류 포함 상황**

   - 입력: 오류 유도 질문
   - 기대: 검증 에이전트에서 오류를 감지하고 재시도 루프 실행됨

5. **출처 누락 유도 테스트**

   - 의도적으로 출처 없는 응답이 생성되도록 설정
   - 기대: 검증 에이전트가 누락 감지 및 수정 유도

---

1. 질문 분석 → 하위 쿼리 자동 생성
2. 병렬 검색 → 뉴스 및 블로그 정보 수집
3. 문서 요약 → 마크다운 구조화 + 출처 포함
4. 검증 에이전트 1회 반복 루프
5. Streamlit UI 최소 구성 + 진행 상태 표시

