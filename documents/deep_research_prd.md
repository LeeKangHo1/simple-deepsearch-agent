# 📄 Product Requirements Document (PRD)

## 💡 프로젝트명: 딥 리서치 챗봇 (Deep Research Chatbot)

---

## 1. 개요 (Overview)

deep research chatbot은 LangChain, LangGraph, LangSmith 기반의 멀티 에이전트 시스템으로, 사용자의 질문에 대해 다양한 관점에서 심층적으로 조사한 결과를 제공합니다. 검색엔진 병렬 활용과 문서 요약, 출처 기반 응답을 통해 신뢰도 높은 정보를 제공하며, **검증 실패 시 자동 재검색 기능**을 통해 품질을 보장합니다. Streamlit UI로 단순하고 명료하게 인터페이스를 구성합니다.

---

## 2. 사용자 정의 (Target User)

- 정보 탐색의 깊이를 원하는 **비전문 일반인 \~ 준연구자**
  - 예: 기획자, 대학생, 작가, 스타트업 창업자 등
- 배경지식은 있으나 논문 검색이나 전문 DB는 부담스러운 사용자층
- ChatGPT와 같은 대화형 인터페이스에 익숙한 사용자

---

## 3. 핵심 기능 (Key Features)

### 3.1 질문 확장 → 다중 쿼리 생성

- 사용자 입력(질문 또는 키워드)을 분석하여 하위 질문(sub-queries) 자동 생성
- LangChain 기반 Prompt Template 또는 LLM을 이용한 확장

### 3.2 병렬 웹 검색 + 벡터 DB 저장 (DuckDuckGo + Tavily + Chroma)

- 생성된 서브 쿼리를 Tavily와 DuckDuckGo에서 병렬 검색
- 뉴스 및 블로그 기반 우선 수집, 논문 및 리포트는 선택적으로 활용
- **수집된 문서들을 임베딩하여 Chroma 벡터 DB에 저장**
- **임베딩 유사도를 활용한 지능적 중복 제거** (URL + 내용 유사도 기반)
- **사용자 질문과의 의미적 유사도로 관련성 높은 문서 우선 정렬**

### 3.3 문서 요약 + 출처 포함 응답 생성

- 수집된 문서를 멀티 문서 요약
- 마크다운 형식으로 구조화된 응답 생성
- 각 문장 옆에 출처 인라인 표기

### 3.4 **스마트 검증 및 재검색 시스템 (핵심 개선사항)**

- 응답 생성 후 검증 에이전트가 논리적 오류, 부정확성, 출처 누락 등을 확인
- **검증 실패 시 자동으로 다른 관점의 쿼리 생성 및 재검색**
  - LLM이 검증 피드백을 분석하여 새로운 관점의 검색어 생성
  - 기존 검색 결과와 새 검색 결과를 자동 병합
  - **최대 1회 재시도로 효율성 보장**
- 중복 제거 및 품질 향상된 최종 응답 제공

### 3.5 진행 상태 출력

- 각 에이전트 단계별 상태를 Streamlit UI 상에 표시
- 재검색 시 별도 상태 메시지 표시
  - 예: 질문 분석 완료 / 웹 검색 중 / 추가 검색 중 / 요약 중 / 검증 중 ...

---

## 4. 사용자 인터페이스 (UI)

- **Streamlit 기반 단일 창 구성**

  - 상단: 텍스트 입력창 (사용자가 질문 입력)
  - 중단: 진행 상태 표시 (에이전트 진행 상황 실시간 업데이트)
  - 하단: 구조화된 마크다운 응답 출력 (출처 포함)

- **UI 표시 항목 최소화**

  - 사용자가 작성한 질문
  - 시스템이 응답한 마크다운 정리본
  - 진행 상태 메시지 (예: "검색 중", "추가 검색 중", "요약 중", "응답 생성 완료")

- **불필요한 기능 배제**

  - 세션 저장, 대화 기록 없음
  - 단일 입력 → 결과 확인 흐름만 유지

### 💡 Streamlit UI 시나리오 예시 (재검색 포함)

1. 사용자가 질문 입력

   - 입력창: `오픈소스 LLM 트렌드 알려줘`

2. 중간 진행 표시 (초기 검색)

   - 메시지:
     ```
     [질문 분석 중...]
     [웹 검색 실행 중 (DuckDuckGo + Tavily)...]
     [문서 요약 중...]
     [최종 응답 생성 중...]
     [응답 품질 검증 중...]
     ```

3. **재검색 시나리오 (검증 실패 시)**

   - 메시지:
     ```
     [검증 결과: 기술적 세부사항 부족으로 추가 검색 필요]
     [새로운 관점의 검색어 생성 중...]
     [추가 검색 실행 중: "오픈소스 LLM 성능 비교", "LLM 상업적 활용"...]
     [기존 결과와 병합 중...]
     [재요약 및 재검증 중...]
     ```

4. 결과 출력

   - 사용자의 질문: `오픈소스 LLM 트렌드 알려줘`
   - 마크다운 응답:
     ```
     ## 오픈소스 LLM 트렌드
     - 최근 공개된 Mistral 모델은 성능이 높고 경량화되었다. *(source: huggingface.co)*
     - Meta의 LLaMA2는 상업적 사용까지 허용되며 영향력이 크다. *(source: arxiv.org)*
     - 기업들이 오픈소스 LLM을 채택하는 이유는 비용보다 유연성 때문이다. *(source: techcrunch.com)*
     ```

---

## 5. 멀티 에이전트 구성 (Agent Architecture)

### 💬 LLM 프롬프트 / 응답 구조 예시

아래는 각 에이전트가 사용하는 LLM 입력 프롬프트와 기대 응답 형식의 예시입니다.

#### 🧾 질문 분석 에이전트

**프롬프트 템플릿:**

```
다음 사용자 질문을 기반으로 조사에 도움이 될 세부 질문을 3~5개 생성해줘:

질문: {user_input}
```

**예상 응답:**

```json
[
  "오픈소스 LLM의 최신 트렌드는 무엇인가요?",
  "Mistral이나 LLaMA와 같은 주요 오픈소스 모델은 어떤 특징이 있나요?"
]
```

#### **🔄 재검색 쿼리 생성 (신규 추가)**

**프롬프트 템플릿:**

```
원본 질문: {question}

기존 검색 쿼리:
{existing_queries}

검증 피드백: {feedback}

위 피드백을 바탕으로 부족한 정보를 보완할 수 있는 새로운 관점의 검색 쿼리를 생성해주세요.
```

**예상 응답:**

```json
[
  "오픈소스 LLM 성능 벤치마크 비교",
  "LLM 기업 도입 사례 분석",
  "오픈소스 LLM 기술적 한계점"
]
```

#### 📚 문서 요약 에이전트

**프롬프트 템플릿:**

```
다음 기사 내용을 3줄 이내로 요약해줘:

문서: {content}
```

#### 🧠 인사이트 에이전트

**프롬프트 템플릿:**

```
다음 요약된 문서들을 기반으로 핵심 인사이트 3가지를 도출해줘:

요약 리스트:
- {summary_1}
- {summary_2} ...
```

#### 🧩 응답 생성 에이전트

**프롬프트 템플릿:**

```
다음 인사이트와 문서 출처를 바탕으로 마크다운 형식의 응답을 생성해줘. 각 문장 옆에 출처를 명시해줘:

인사이트: {insights}
문서 리스트: {documents}
```

#### **✅ 검증 에이전트 (강화)**

**프롬프트 템플릿:**

```
다음 응답을 검토하여 품질을 평가해주세요:

원본 질문: {question}
검증할 응답: {response}

검증 기준:
1. 사용자 질문에 적절히 답변했는가?
2. 논리적 일관성이 있는가?
3. 출처 표시가 적절히 되어 있는가?
4. 구체적인 정보와 데이터가 충분한가?
5. 마크다운 형식이 올바른가?

출력 형식:
{
  "is_valid": true/false,
  "feedback": "구체적인 피드백",
  "missing_aspects": ["부족한 관점1", "부족한 관점2"]
}
```

### 🧾 상태(State) 구조 정의 (업데이트)

LangGraph 전체 워크플로우에서 공유되는 상태 객체:

```python
State = {
  "user_input": str,                  # 사용자 입력 원문
  "sub_queries": List[str],           # 질문 분석 결과 생성된 하위 쿼리 목록
  "documents": List[Dict],            # 검색 결과로 수집된 문서 리스트 (병합됨)
  "summaries": List[str],             # 각 문서 요약 결과
  "insights": List[str],              # 인사이트 도출 결과
  "markdown_answer": str,             # 마크다운 형식 응답 (출처 포함)
  "is_valid": bool,                   # 검증 결과 여부
  "validation_feedback": str,         # 검증 피드백 (재검색 방향 제시)
  "retry_count": int,                 # 검증 실패로 인한 재시도 횟수 (최대 1회)
  "current_step": str,                # 현재 처리 단계
  "logs": List[str],                  # 진행 상태 로깅
  "step_timestamps": Dict[str, float] # 각 단계별 시작 시간
}
```

### 📥 LangGraph 노드별 입력/출력 구조 (업데이트)

| 노드 이름                | 입력 데이터                | 출력 데이터                                          | 특이사항 |
| -------------------- | --------------------- | ----------------------------------------------- | ---- |
| `input_node`         | 사용자 자연어 질문 (str)      | 질문 (str)                                        |      |
| `question_analyzer`  | 질문 (str)              | 하위 쿼리 리스트 (List[str])                           |      |
| `web_search`         | 하위 쿼리 리스트 (List[str]) | 문서 리스트 (List[Dict])                             | **재시도시 기존+새 결과 병합** |
| `doc_summarizer`     | 문서 리스트                | 요약 리스트 (List[str])                              |      |
| `insight_generator`  | 요약 리스트                | 인사이트 리스트 (List[str])                            |      |
| `response_generator` | 인사이트 리스트, 문서 리스트      | 마크다운 응답 (str)                                   |      |
| `response_validator` | 마크다운 응답              | 검증 결과 (bool), 피드백 (str)                        | **재검색 방향 제시** |
| `final_output`       | 응답 결과 (str)           | 사용자에게 출력될 최종 결과                                 |      |

- **노드 정의:**

  - `input_node` → 사용자 질문 입력 노드
  - `question_analyzer` → 질문 분석 에이전트 노드
  - `web_search` → 병렬 검색 실행 노드 (**재검색 로직 포함**)
  - `doc_summarizer` → 문서 요약 노드
  - `insight_generator` → 인사이트 생성 노드
  - `response_generator` → 응답 생성 노드
  - `response_validator` → 응답 검증 노드 (**강화된 검증**)
  - `final_output` → 결과 출력 노드

- **노드 연결 흐름 (업데이트):**

  ```
  input_node → question_analyzer → web_search → doc_summarizer → insight_generator → response_generator → response_validator

  response_validator →
      ├── final_output (if valid OR retry_count >= 1)
      └── web_search (if invalid AND retry_count < 1) ← 재검색 루프
  ```

### 🔁 **에이전트 간 데이터 흐름도 (업데이트)**

```
[사용자 입력]
     │
     ▼
[🧾 질문 분석 에이전트]
 - 입력된 질문을 분석하여 하위 쿼리(서브 질문)들을 생성
     │
     ▼
[🌐 웹 검색 에이전트] ← ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐
 - DuckDuckGo + Tavily에서 병렬 검색 수행                              │
 - 각 하위 쿼리에 대해 웹 검색 결과 수집                                   │
 - **재시도시: LLM으로 새 관점 쿼리 생성 + 기존 결과와 병합**                  │
     │                                                              │
     ▼                                                              │
[📚 문서 요약 에이전트]                                                   │
 - 수집된 문서를 각각 요약                                               │
 - 핵심 정보만 추출                                                    │
     │                                                              │
     ▼                                                              │
[🧠 인사이트 에이전트]                                                    │
 - 요약된 문서들로부터 인사이트(시사점, 핵심 논점) 도출                          │
     │                                                              │
     ▼                                                              │
[🧩 응답 생성 에이전트]                                                   │
 - 전체 내용을 마크다운 구조로 정리                                         │
 - 각 문장 옆에 출처 포함                                               │
     │                                                              │
     ▼                                                              │
[✅ 검증 에이전트] **강화됨**                                             │
 - 최종 응답을 검토 (논리 오류, 출처 누락, 정보 부족 등)                       │
     ├── 문제가 없으면 → [최종 출력]                                      │
     └── 문제가 있고 재시도 가능하면 → **새로운 관점 쿼리 생성 요청** ─ ─ ─ ─ ─ ┘
         (검증 피드백을 웹 검색 에이전트에 전달)
```

---

## 6. 기술 스택 (Tech Stack)

| 영역            | 기술                                                                                                                                     |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| 멀티 에이전트 프레임워크 | LangChain + LangGraph                                                                                                                  |
| 검색 API        | DuckDuckGo, Tavily                                                                                                                     |
| LLM 및 임베딩     | GPT 계열(GPT-4o, GPT-4o-mini, text-embedding-3-large) 또는 Gemini 계열(Gemini 2.5 Flash, Gemini 2.5 Pro, gemini-embedding-001) 중 통일 사용 |
| 벡터 DB         | Chroma                                                                                                                                 |
| 인터페이스         | Streamlit                                                                                                                              |
| 환경 변수 관리      | `.env` 파일 기반 구성 (API 키, LLM 종류 등)                                                                                                      |
| 실행 환경         | Python 가상환경 (venv 또는 poetry 등)에서 로컬 실행                                                                                                 |
| 모니터링          | LangSmith (설정만 적용, 사용자는 직접 확인)                                                                                                         |

---

## 7. MVP 범위 (Minimum Viable Product)

1. 질문 분석 → 하위 쿼리 자동 생성
2. 병렬 검색 → 뉴스 및 블로그 정보 수집
3. 문서 요약 → 마크다운 구조화 + 출처 포함
4. **스마트 검증 + 1회 재검색 시스템** (**핵심 기능**)
5. Streamlit UI 최소 구성 + 진행 상태 표시 (**재검색 상태 포함**)

---

## 8. **핵심 개선사항 요약**

### 🎯 **자동 품질 보장 시스템**
- **검증 실패 시 자동 재검색**: 사용자 개입 없이 품질 문제 해결
- **LLM 기반 쿼리 다각화**: 기존 검색으로 부족한 관점을 자동 보완
- **결과 병합 및 중복 제거**: 기존 + 새 검색 결과를 지능적으로 통합

### 📊 **효율적인 재시도 정책**
- **최대 1회 재시도**: 무한 루프 방지 및 응답 시간 보장
- **구체적 피드백 활용**: 단순 재시도가 아닌 목적성 있는 추가 검색
- **점진적 정보 축적**: 기존 정보를 버리지 않고 보완하는 방식

### 🚀 **사용자 경험 향상**
- **투명한 진행 상태**: 재검색 과정을 사용자에게 실시간 공개
- **안정적인 결과 제공**: 재검색 실패 시에도 기존 결과로 응답 보장
- **품질 기반 자동 최적화**: 수동 개입 없이 결과 품질 자동 향상

---

## 🔧 개발 환경 메모

- `.env` 예시

  ```env
  LLM_PROVIDER=gpt
  OPENAI_API_KEY=sk-...
  GEMINI_API_KEY=...
  ```

- 실행 예시 (가상환경 기준)

  ```bash
  source venv/bin/activate
  streamlit run main.py
  ```

## 9. 평가 지표 및 테스트 시나리오

### 🎯 평가 지표 (Evaluation Metrics)

| 항목          | 설명                               | 기준                      |
| ----------- | -------------------------------- | ----------------------- |
| 응답 구조화 완성도  | 마크다운 응답이 명확한 구조로 작성되어 있는가        | 제목/소제목/글머리표 존재 여부로 체크   |
| 출처 명확성      | 각 문장 또는 인사이트에 출처가 잘 달려 있는가       | 출처 누락 문장 1개 이하           |
| 중복 제거 정확도   | 검색 결과에서 유사 문서가 제거되었는가           | 중복 문서 10% 이하            |
| 인사이트 품질     | 단순 요약이 아닌 새로운 관찰/시사점이 있는가       | 사람 평가 또는 예시 답안 비교       |
| **재검색 효과성** | **재검색을 통해 응답 품질이 실제로 향상되었는가**   | **재검색 전후 품질 점수 비교**     |
| **시스템 안정성** | **재검색 실패 시에도 안정적으로 응답을 제공하는가** | **오류 발생률 5% 이하**       |

### 🧪 테스트 시나리오 (Test Scenarios)

1. **기본 사용 흐름**
   - 입력: "오픈소스 LLM 트렌드 알려줘"
   - 기대: 질문 확장 → 검색 → 요약 → 응답 생성 → 검증 통과 → 최종 출력

2. **재검색 트리거 시나리오**
   - 입력: "블록체인의 미래" (의도적으로 모호한 질문)
   - 기대: 초기 검색 → 검증 실패 → 구체적 관점 재검색 → 품질 향상된 응답

3. **재검색 후 병합 확인**
   - 기대: 기존 문서 + 새 문서가 중복 없이 병합됨
   - 검증: 문서 수 증가 및 내용 다양성 확인

4. **재검색 한계 테스트**
   - 입력: 매우 특수한 주제로 정보가 부족한 질문
   - 기대: 1회 재검색 후 실패해도 기존 결과로 응답 제공

5. **검증 품질 향상 확인**
   - 재검색 전후 응답을 인간 평가자가 비교
   - 기대: 재검색 후 구체성, 정확성, 완성도 향상