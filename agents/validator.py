# agents/validator.py
"""
ê²€ì¦ ì—ì´ì „íŠ¸

ìƒì„±ëœ ìµœì¢… ì‘ë‹µì˜ í’ˆì§ˆ, ë…¼ë¦¬ì„±, ì™„ì„±ë„ë¥¼ ê²€ì¦í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ê¸° ì „ì—
í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤. ê²€ì¦ ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì ì¸ í”¼ë“œë°±ê³¼ ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì‘ë‹µ êµ¬ì¡° ë° í˜•ì‹ ê²€ì¦
- ì¶œì²˜ í‘œì‹œ ì™„ì„±ë„ í™•ì¸
- ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦
- ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ í‰ê°€
- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í™•ì¸
- ê²€ì¦ ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì  í”¼ë“œë°± ì œê³µ
"""

import re
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.data_models import ValidationResult
from models.state import ResearchState, StateManager
from services.llm_service import get_llm_service, LLMResponse
from utils.text_processing import clean_text, count_words, extract_keywords
from utils.validators import validate_response_structure, validate_insights
from utils.logger import create_agent_logger, log_execution_time

logger = logging.getLogger(__name__)
agent_logger = create_agent_logger("validator")

class ResponseValidator:
    """
    ì‘ë‹µ ê²€ì¦ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    ìƒì„±ëœ ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì˜ ì „ë°˜ì ì¸ í’ˆì§ˆì„ ë‹¤ê°ë„ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
    êµ¬ì¡°ì  ì™„ì„±ë„ë¶€í„° ë‚´ìš©ì˜ ë…¼ë¦¬ì„±ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬
    ê³ í’ˆì§ˆ ì‘ë‹µë§Œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
    
    ê²€ì¦ í•­ëª©:
    1. êµ¬ì¡°ì  ì™„ì„±ë„ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹, ì„¹ì…˜ êµ¬ì¡°)
    2. ì¶œì²˜ í‘œì‹œ ì™„ì„±ë„ (ëˆ„ë½, í˜•ì‹ ì˜¤ë¥˜)
    3. ë‚´ìš© í’ˆì§ˆ (ë…¼ë¦¬ì„±, ì¼ê´€ì„±, ê´€ë ¨ì„±)
    4. ì¸ì‚¬ì´íŠ¸ í¬í•¨ ì—¬ë¶€ (ì›ë³¸ ëŒ€ë¹„ ëˆ„ë½ ê²€ì¦)
    5. ì‚¬ìš©ì ì§ˆë¬¸ ì‘ë‹µ ì í•©ì„±
    """
    
    def __init__(self, max_retry_count: int = 2):
        """
        ê²€ì¦ê¸° ì´ˆê¸°í™”
        
        Args:
            max_retry_count: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 2íšŒ)
        """
        self.max_retry_count = max_retry_count
        self.llm_service = get_llm_service()
        
        # ê²€ì¦ ê¸°ì¤€ ì„¤ì • (ì™„í™”ë¨)
        self.validation_criteria = {
            "min_word_count": 100,        # ìµœì†Œ ë‹¨ì–´ ìˆ˜ (200 â†’ 100)
            "max_word_count": 10000,      # ìµœëŒ€ ë‹¨ì–´ ìˆ˜
            "required_sections": ["ì¸ì‚¬ì´íŠ¸"],  # í•„ìˆ˜ ì„¹ì…˜ ì™„í™” (ì¸ì‚¬ì´íŠ¸ë§Œ í•„ìˆ˜)
            "min_insights_ratio": 0.5,    # ì›ë³¸ ì¸ì‚¬ì´íŠ¸ ëŒ€ë¹„ ìµœì†Œ í¬í•¨ ë¹„ìœ¨ (0.8 â†’ 0.5)
            "min_sources_count": 1,       # ìµœì†Œ ì¶œì²˜ ê°œìˆ˜
        }
        
        # í†µê³„ ì¶”ì 
        self.total_validations = 0
        self.passed_validations = 0
        self.failed_validations = 0
        self.llm_validation_count = 0
    
    @log_execution_time
    async def validate_response(
        self,
        response_content: str,
        user_question: str,
        original_insights: List[str],
        original_sources: List[Dict[str, str]]
    ) -> ValidationResult:
        """
        ì‘ë‹µ í’ˆì§ˆ ì¢…í•© ê²€ì¦
        
        Args:
            response_content: ê²€ì¦í•  ì‘ë‹µ ë‚´ìš©
            user_question: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
            original_insights: ì›ë³¸ ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            original_sources: ì›ë³¸ ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ValidationResult: ê²€ì¦ ê²°ê³¼ ê°ì²´
        """
        if not response_content or not response_content.strip():
            return ValidationResult(
                is_valid=False,
                feedback="ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                issues=["ë¹ˆ ì‘ë‹µ"],
                confidence_score=0.0
            )
        
        agent_logger.start_step("ì‘ë‹µ ê²€ì¦ ì‹œì‘")
        self.total_validations += 1
        
        try:
            # 1ë‹¨ê³„: êµ¬ì¡°ì  ê²€ì¦ (ë¹ ë¥¸ ê²€ì¦)
            structural_issues = self._validate_structure(response_content)
            
            # 2ë‹¨ê³„: ì¶œì²˜ ê²€ì¦
            source_issues = self._validate_sources(response_content, original_sources)
            
            # 3ë‹¨ê³„: ì¸ì‚¬ì´íŠ¸ í¬í•¨ ê²€ì¦
            insight_issues = self._validate_insights_inclusion(response_content, original_insights)
            
            # 4ë‹¨ê³„: ë‚´ìš© í’ˆì§ˆ ê²€ì¦ (LLM ê¸°ë°˜)
            content_validation = await self._validate_content_quality(response_content, user_question)
            
            # 5ë‹¨ê³„: ì¢…í•© í‰ê°€
            all_issues = structural_issues + source_issues + insight_issues
            if not content_validation.success:
                all_issues.extend(content_validation.content.get("issues", []))
            
            # ê²€ì¦ ê²°ê³¼ ê²°ì • (ì™„í™”ëœ ê¸°ì¤€)
            # ì‹¬ê°í•œ ë¬¸ì œê°€ ì—†ìœ¼ë©´ í†µê³¼ (ê²½ë¯¸í•œ ë¬¸ì œëŠ” í—ˆìš©)
            critical_issues = [issue for issue in all_issues if any(
                critical in issue.lower() for critical in ["ë¹ˆ ì‘ë‹µ", "ê²€ì¦ ì˜¤ë¥˜", "íŒŒì‹± ì‹¤íŒ¨"]
            )]
            
            is_valid = len(critical_issues) == 0  # ì‹¬ê°í•œ ë¬¸ì œë§Œ ì²´í¬
            confidence_score = self._calculate_confidence_score(
                len(all_issues), content_validation, response_content
            )
            
            # í”¼ë“œë°± ìƒì„±
            feedback = self._generate_feedback(all_issues, content_validation)
            suggestions = self._generate_suggestions(all_issues, user_question)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if is_valid:
                self.passed_validations += 1
            else:
                self.failed_validations += 1
            
            result = ValidationResult(
                is_valid=is_valid,
                feedback=feedback,
                issues=all_issues,
                suggestions=suggestions,
                confidence_score=confidence_score
            )
            
            agent_logger.end_step(
                "ì‘ë‹µ ê²€ì¦ ì™„ë£Œ",
                is_valid,
                f"ì‹ ë¢°ë„: {confidence_score:.2f}, ì´ìŠˆ: {len(all_issues)}ê°œ"
            )
            
            return result
            
        except Exception as e:
            agent_logger.end_step("ì‘ë‹µ ê²€ì¦", False, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ì‘ë‹µ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            return ValidationResult(
                is_valid=False,
                feedback=f"ê²€ì¦ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                issues=["ê²€ì¦ ì˜¤ë¥˜"],
                confidence_score=0.0
            )
    
    def _validate_structure(self, content: str) -> List[str]:
        """
        ì‘ë‹µ êµ¬ì¡° ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ì‘ë‹µ ë‚´ìš©
            
        Returns:
            List[str]: ë°œê²¬ëœ êµ¬ì¡°ì  ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
        """
        issues = []
        
        try:
            # ê¸°ë³¸ ê¸¸ì´ ê²€ì¦
            word_stats = count_words(content)
            total_chars = word_stats["total_chars"]
            
            if total_chars < self.validation_criteria["min_word_count"]:
                issues.append(f"ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({total_chars}ì, ìµœì†Œ {self.validation_criteria['min_word_count']}ì í•„ìš”)")
            
            if total_chars > self.validation_criteria["max_word_count"]:
                issues.append(f"ì‘ë‹µì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({total_chars}ì, ìµœëŒ€ {self.validation_criteria['max_word_count']}ì)")
            
            # ë§ˆí¬ë‹¤ìš´ ì œëª© êµ¬ì¡° í™•ì¸
            if not re.search(r'^#\s+.+', content, re.MULTILINE):
                issues.append("ë©”ì¸ ì œëª©(# )ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # í•„ìˆ˜ ì„¹ì…˜ í™•ì¸
            missing_sections = []
            for section in self.validation_criteria["required_sections"]:
                section_pattern = rf'##\s+{re.escape(section)}'
                if not re.search(section_pattern, content):
                    missing_sections.append(section)
            
            if missing_sections:
                issues.append(f"í•„ìˆ˜ ì„¹ì…˜ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_sections)}")
            
            # ë¶ˆë¦¿ í¬ì¸íŠ¸ ë˜ëŠ” êµ¬ì¡°í™”ëœ ë‚´ìš© í™•ì¸
            structure_patterns = [
                r'^\s*[-*+]\s+.+',      # ë¶ˆë¦¿ í¬ì¸íŠ¸
                r'^\s*\d+\.\s+.+',      # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
                r'^#{2,}\s+.+',         # ì†Œì œëª©
            ]
            
            has_structure = any(
                re.search(pattern, content, re.MULTILINE) 
                for pattern in structure_patterns
            )
            
            if not has_structure:
                issues.append("êµ¬ì¡°í™”ëœ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ë¶ˆë¦¿í¬ì¸íŠ¸, ì†Œì œëª© ë“±)")
            
            return issues
            
        except Exception as e:
            logger.error(f"êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return ["êµ¬ì¡° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"]
    
    def _validate_sources(self, content: str, original_sources: List[Dict[str, str]]) -> List[str]:
        """
        ì¶œì²˜ í‘œì‹œ ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ì‘ë‹µ ë‚´ìš©
            original_sources: ì›ë³¸ ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ë°œê²¬ëœ ì¶œì²˜ ê´€ë ¨ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
        """
        issues = []
        
        try:
            # ì¶œì²˜ í‘œì‹œ íŒ¨í„´ í™•ì¸
            source_patterns = [
                r'\*\([^)]*ì¶œì²˜[^)]*\)\*',           # *(ì¶œì²˜: domain.com)*
                r'\*\([^)]*source[^)]*\)\*',         # *(source: domain.com)*
                r'#{3,}\s*ì¶œì²˜',                     # ### ì¶œì²˜
                r'---[^#]*ì¶œì²˜',                     # --- ì¶œì²˜
            ]
            
            has_sources = any(
                re.search(pattern, content, re.IGNORECASE) 
                for pattern in source_patterns
            )
            
            if not has_sources:
                issues.append("ì¶œì²˜ í‘œì‹œê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì›ë³¸ ì¶œì²˜ ê°œìˆ˜ì™€ ë¹„êµ
            if len(original_sources) >= self.validation_criteria["min_sources_count"]:
                # ë„ë©”ì¸ ì¶”ì¶œí•˜ì—¬ í¬í•¨ ì—¬ë¶€ í™•ì¸
                content_lower = content.lower()
                found_domains = 0
                
                for source in original_sources[:5]:  # ìƒìœ„ 5ê°œë§Œ í™•ì¸
                    domain = source.get("domain", "").lower()
                    if domain and domain in content_lower:
                        found_domains += 1
                
                if found_domains == 0:
                    issues.append("ì›ë³¸ ì¶œì²˜ê°€ ì‘ë‹µì— ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                elif found_domains < len(original_sources) * 0.5:
                    issues.append("ì¼ë¶€ ì¶œì²˜ë§Œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            return issues
            
        except Exception as e:
            logger.error(f"ì¶œì²˜ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return ["ì¶œì²˜ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"]
    
    def _validate_insights_inclusion(self, content: str, original_insights: List[str]) -> List[str]:
        """
        ì¸ì‚¬ì´íŠ¸ í¬í•¨ ì—¬ë¶€ ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ì‘ë‹µ ë‚´ìš©
            original_insights: ì›ë³¸ ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ë°œê²¬ëœ ì¸ì‚¬ì´íŠ¸ ê´€ë ¨ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
        """
        issues = []
        
        try:
            if not original_insights:
                return issues  # ì›ë³¸ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê²€ì¦ ë¶ˆê°€
            
            content_lower = content.lower()
            included_insights = 0
            
            for insight in original_insights:
                # ì¸ì‚¬ì´íŠ¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë“¤ë¡œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                insight_keywords = extract_keywords(insight, max_keywords=3, min_length=3)
                
                if not insight_keywords:
                    continue
                
                # í‚¤ì›Œë“œ ì¤‘ ì¼ì • ë¹„ìœ¨ ì´ìƒì´ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                matching_keywords = sum(
                    1 for keyword in insight_keywords 
                    if keyword.lower() in content_lower
                )
                
                keyword_ratio = matching_keywords / len(insight_keywords)
                if keyword_ratio >= 0.5:  # 50% ì´ìƒ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ë©´ í¬í•¨ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                    included_insights += 1
            
            inclusion_ratio = included_insights / len(original_insights)
            min_ratio = self.validation_criteria["min_insights_ratio"]
            
            if inclusion_ratio < min_ratio:
                issues.append(
                    f"ì¸ì‚¬ì´íŠ¸ í¬í•¨ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ "
                    f"({included_insights}/{len(original_insights)}, {inclusion_ratio:.1%})"
                )
            
            # ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ ìì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            if not re.search(r'##\s*ì£¼ìš”\s*ì¸ì‚¬ì´íŠ¸', content, re.IGNORECASE):
                issues.append("ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
            
            return issues
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return ["ì¸ì‚¬ì´íŠ¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"]
    
    async def _validate_content_quality(self, content: str, user_question: str) -> LLMResponse:
        """
        LLMì„ í†µí•œ ë‚´ìš© í’ˆì§ˆ ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ì‘ë‹µ ë‚´ìš©
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            LLMResponse: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        self.llm_validation_count += 1
        
        # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (í† í° ì ˆì•½)
        truncated_content = content[:3000] if len(content) > 3000 else content
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‘ë‹µì„ ê²€í† í•˜ì—¬ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ê²€ì¦ ê¸°ì¤€:
1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ë…¼ë¦¬ì  ì¼ê´€ì„±ì´ ìˆëŠ”ê°€?
3. ë‚´ìš©ì´ ê°ê´€ì ì´ê³  ì •í™•í•œê°€?
4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?
5. ì •ë³´ê°€ ì˜ êµ¬ì¡°í™”ë˜ì–´ ìˆëŠ”ê°€?

ì¶œë ¥ í˜•ì‹ (JSON):
{{
    "is_valid": true/false,
    "feedback": "êµ¬ì²´ì ì¸ í”¼ë“œë°± ë©”ì‹œì§€",
    "issues": ["ë¬¸ì œì 1", "ë¬¸ì œì 2", ...],
    "suggestions": ["ê°œì„ ì‚¬í•­1", "ê°œì„ ì‚¬í•­2", ...],
    "content_score": 0.0-1.0,
    "relevance_score": 0.0-1.0,
    "structure_score": 0.0-1.0
}}

ê° ì ìˆ˜ëŠ” 0.0(ë§¤ìš° ë‚˜ì¨)ë¶€í„° 1.0(ë§¤ìš° ì¢‹ìŒ)ê¹Œì§€ì…ë‹ˆë‹¤."""),
            ("user", "ì›ë³¸ ì§ˆë¬¸: {question}\n\nê²€ì¦í•  ì‘ë‹µ:\n{response}")
        ])
        
        return await self.llm_service._call_llm(
            prompt_template=prompt_template,
            input_variables={"question": user_question, "response": truncated_content},
            agent_type="validator",
            output_parser=self.llm_service.json_parser,
            expected_type=dict
        )
    
    def _calculate_confidence_score(
        self,
        issues_count: int,
        content_validation: LLMResponse,
        response_content: str
    ) -> float:
        """
        ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            issues_count: ë°œê²¬ëœ ë¬¸ì œì  ê°œìˆ˜
            content_validation: LLM í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
            response_content: ì‘ë‹µ ë‚´ìš©
            
        Returns:
            float: ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            # ê¸°ë³¸ ì ìˆ˜ì—ì„œ ë¬¸ì œì ë§Œí¼ ì°¨ê°
            base_score = 1.0
            
            # ë¬¸ì œì  ê°œìˆ˜ì— ë”°ë¥¸ ì°¨ê° (ë¬¸ì œ 1ê°œë‹¹ -0.2ì )
            issues_penalty = min(issues_count * 0.2, 0.8)
            base_score -= issues_penalty
            
            # LLM ê²€ì¦ ê²°ê³¼ ë°˜ì˜
            if content_validation.success:
                try:
                    validation_data = content_validation.content
                    if isinstance(validation_data, str):
                        import json
                        validation_data = json.loads(validation_data)
                    
                    # LLMì´ ì œê³µí•œ ì ìˆ˜ë“¤ í‰ê· 
                    llm_scores = [
                        validation_data.get("content_score", 0.7),
                        validation_data.get("relevance_score", 0.7),
                        validation_data.get("structure_score", 0.7)
                    ]
                    llm_avg_score = sum(llm_scores) / len(llm_scores)
                    
                    # ê¸°ë³¸ ì ìˆ˜ì™€ LLM ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· 
                    final_score = (base_score * 0.6) + (llm_avg_score * 0.4)
                    
                except Exception as e:
                    logger.warning(f"LLM ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    final_score = base_score * 0.8  # LLM ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì  ì ìˆ˜
            else:
                final_score = base_score * 0.5  # LLM ê²€ì¦ ì‹¤íŒ¨ ì‹œ í° íë„í‹°
            
            # ì‘ë‹µ ê¸¸ì´ ë³´ë„ˆìŠ¤ (ì ë‹¹í•œ ê¸¸ì´ì— ë³´ë„ˆìŠ¤)
            content_length = len(response_content)
            if 500 <= content_length <= 5000:
                length_bonus = 0.1
            else:
                length_bonus = 0.0
            
            final_score += length_bonus
            
            # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œ
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _generate_feedback(self, issues: List[str], content_validation: LLMResponse) -> str:
        """
        ê²€ì¦ í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±
        
        Args:
            issues: ë°œê²¬ëœ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
            content_validation: LLM ê²€ì¦ ê²°ê³¼
            
        Returns:
            str: í”¼ë“œë°± ë©”ì‹œì§€
        """
        if not issues and content_validation.success:
            return "ì‘ë‹µ í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ëª¨ë“  ê²€ì¦ ê¸°ì¤€ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
        
        feedback_parts = []
        
        # êµ¬ì¡°ì  ë¬¸ì œì 
        structural_issues = [issue for issue in issues if any(
            keyword in issue for keyword in ["ì œëª©", "ì„¹ì…˜", "êµ¬ì¡°", "í˜•ì‹", "ê¸¸ì´"]
        )]
        if structural_issues:
            feedback_parts.append(f"êµ¬ì¡°ì  ê°œì„  í•„ìš”: {'; '.join(structural_issues[:2])}")
        
        # ë‚´ìš© ë¬¸ì œì 
        content_issues = [issue for issue in issues if issue not in structural_issues]
        if content_issues:
            feedback_parts.append(f"ë‚´ìš© ê°œì„  í•„ìš”: {'; '.join(content_issues[:2])}")
        
        # LLM í”¼ë“œë°± ì¶”ê°€
        if content_validation.success:
            try:
                validation_data = content_validation.content
                if isinstance(validation_data, str):
                    import json
                    validation_data = json.loads(validation_data)
                
                llm_feedback = validation_data.get("feedback", "")
                if llm_feedback and len(llm_feedback) < 200:
                    feedback_parts.append(f"ì¶”ê°€ ì˜ê²¬: {llm_feedback}")
                    
            except Exception as e:
                logger.warning(f"LLM í”¼ë“œë°± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return " | ".join(feedback_parts) if feedback_parts else "ê²€ì¦ì—ì„œ ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def _generate_suggestions(self, issues: List[str], user_question: str) -> List[str]:
        """
        ê°œì„  ì œì•ˆ ìƒì„±
        
        Args:
            issues: ë°œê²¬ëœ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            List[str]: ê°œì„  ì œì•ˆ ë¦¬ìŠ¤íŠ¸
        """
        suggestions = []
        
        # ë¬¸ì œì  ìœ í˜•ë³„ ì œì•ˆ
        for issue in issues:
            if "ì„¹ì…˜" in issue and "ëˆ„ë½" in issue:
                suggestions.append("ëˆ„ë½ëœ ì„¹ì…˜ì„ ì¶”ê°€í•˜ê³  ê° ì„¹ì…˜ì— ì ì ˆí•œ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”")
            elif "ì¶œì²˜" in issue:
                suggestions.append("ê° ì„¹ì…˜ì— ê´€ë ¨ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³  ì¶œì²˜ ëª©ë¡ì„ ì¶”ê°€í•˜ì„¸ìš”")
            elif "ì¸ì‚¬ì´íŠ¸" in issue:
                suggestions.append("ìƒì„±ëœ ëª¨ë“  ì¸ì‚¬ì´íŠ¸ë¥¼ ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ì— í¬í•¨í•˜ì„¸ìš”")
            elif "ê¸¸ì´" in issue:
                if "ì§§ìŠµë‹ˆë‹¤" in issue:
                    suggestions.append("ë‚´ìš©ì„ ë” ìƒì„¸íˆ ì„¤ëª…í•˜ê³  ì¶”ê°€ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”")
                else:
                    suggestions.append("í•µì‹¬ ë‚´ìš©ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•˜ì„¸ìš”")
            elif "êµ¬ì¡°" in issue:
                suggestions.append("ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ê³  ì„¹ì…˜ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”")
        
        # ì¼ë°˜ì ì¸ ì œì•ˆ ì¶”ê°€
        if not suggestions:
            suggestions.append("ì‘ë‹µì˜ ë…¼ë¦¬ì  íë¦„ì„ ê°œì„ í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë” ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”")
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ê´€ë ¨ ì œì•ˆ
        if user_question:
            question_keywords = extract_keywords(user_question, max_keywords=3)
            if question_keywords:
                suggestions.append(f"'{', '.join(question_keywords)}'ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ë” ê°•ì¡°í•˜ì„¸ìš”")
        
        return suggestions[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """
        ì²˜ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        pass_rate = 0.0
        if self.total_validations > 0:
            pass_rate = (self.passed_validations / self.total_validations) * 100
        
        return {
            "total_validations": self.total_validations,
            "passed_validations": self.passed_validations,
            "failed_validations": self.failed_validations,
            "pass_rate_percent": round(pass_rate, 2),
            "llm_validation_count": self.llm_validation_count,
            "max_retry_count": self.max_retry_count,
            "validation_criteria": self.validation_criteria
        }


# LangGraph ë…¸ë“œ í•¨ìˆ˜
async def validate_response_node(state: ResearchState) -> ResearchState:
    """
    ì‘ë‹µ ê²€ì¦ LangGraph ë…¸ë“œ
    
    Stateì˜ markdown_answerë¥¼ ê²€ì¦í•˜ê³  is_valid, validation_feedbackì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
        
    Returns:
        ResearchState: ê²€ì¦ ê²°ê³¼ê°€ ì¶”ê°€ëœ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    logger.info("=== ì‘ë‹µ ê²€ì¦ ë…¸ë“œ ì‹œì‘ ===")
    
    try:
        # í˜„ì¬ ë‹¨ê³„ ì„¤ì •
        state = StateManager.set_step(state, "validating", "âœ… ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        # ê²€ì¦í•  ë°ì´í„° í™•ì¸
        response_content = state.get("markdown_answer", "")
        if not response_content:
            error_msg = "ê²€ì¦í•  ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."
            logger.warning(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘
        user_question = state.get("user_input", "")
        original_insights = state.get("insights", [])
        original_documents = state.get("documents", [])
        
        # ì›ë³¸ ì¶œì²˜ ì •ë³´ êµ¬ì„±
        original_sources = []
        for doc in original_documents:
            if isinstance(doc, dict) and "url" in doc and "title" in doc:
                domain = doc.get("url", "").split("//")[-1].split("/")[0] if doc.get("url") else "unknown"
                original_sources.append({
                    "title": doc.get("title", ""),
                    "url": doc.get("url", ""),
                    "domain": domain
                })
        
        # ê²€ì¦ê¸° ìƒì„± ë° ì‹¤í–‰
        validator = ResponseValidator()
        validation_result = await validator.validate_response(
            response_content=response_content,
            user_question=user_question,
            original_insights=original_insights,
            original_sources=original_sources
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        new_state = state.copy()
        new_state["is_valid"] = validation_result.is_valid
        new_state["validation_feedback"] = validation_result.feedback
        
        # ì²˜ë¦¬ í†µê³„ ë¡œê¹…
        stats = validator.get_processing_stats()
        logger.info(f"ì‘ë‹µ ê²€ì¦ ì™„ë£Œ: {stats}")
        
        # ê²€ì¦ ê²°ê³¼ì— ë”°ë¥¸ ë¡œê·¸ ë©”ì‹œì§€
        if validation_result.is_valid:
            new_state = StateManager.add_log(
                new_state,
                f"âœ… ì‘ë‹µ ê²€ì¦ í†µê³¼: ì‹ ë¢°ë„ {validation_result.confidence_score:.2f}"
            )
            logger.info("ì‘ë‹µ ê²€ì¦ í†µê³¼")
        else:
            new_state = StateManager.add_log(
                new_state,
                f"âŒ ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {len(validation_result.issues)}ê°œ ë¬¸ì œ ë°œê²¬"
            )
            logger.warning(f"ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {validation_result.feedback}")
            
            # ê°œì„  ì œì•ˆ ë¡œê¹…
            if validation_result.suggestions:
                new_state = StateManager.add_log(
                    new_state,
                    f"ğŸ’¡ ê°œì„  ì œì•ˆ: {'; '.join(validation_result.suggestions[:2])}"
                )
        
        logger.info("=== ì‘ë‹µ ê²€ì¦ ë…¸ë“œ ì™„ë£Œ ===")
        return new_state
        
    except Exception as e:
        error_msg = f"ì‘ë‹µ ê²€ì¦ ë…¸ë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return StateManager.set_error(state, error_msg)


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_response_validator(max_retries: int = 2) -> ResponseValidator:
    """
    ì‘ë‹µ ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        ResponseValidator: ì„¤ì •ëœ ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    return ResponseValidator(max_retry_count=max_retries)

def should_retry_generation(validation_result: ValidationResult, current_retry: int, max_retries: int) -> bool:
    """
    ì¬ìƒì„± ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜
    
    ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ì¬ìƒì„±í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.
    ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš°ì—ë§Œ ì¬ì‹œë„ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    Args:
        validation_result: ê²€ì¦ ê²°ê³¼
        current_retry: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        bool: ì¬ìƒì„±ì´ í•„ìš”í•˜ë©´ True
    """
    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
    if current_retry >= max_retries:
        logger.info(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {current_retry}/{max_retries}")
        return False
    
    # ê²€ì¦ í†µê³¼
    if validation_result.is_valid:
        logger.info("ê²€ì¦ í†µê³¼ - ì¬ì‹œë„ ë¶ˆí•„ìš”")
        return False
    
    # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¬ì‹œë„
    if validation_result.confidence_score < 0.3:
        logger.info(f"ì‹ ë¢°ë„ ë„ˆë¬´ ë‚®ìŒ ({validation_result.confidence_score:.2f}) - ì¬ì‹œë„ í•„ìš”")
        return True
    
    # ì‹¬ê°í•œ êµ¬ì¡°ì  ë¬¸ì œê°€ ìˆìœ¼ë©´ ì¬ì‹œë„
    critical_issues = [
        "ë©”ì¸ ì œëª©",
        "í•„ìˆ˜ ì„¹ì…˜",
        "ì¶œì²˜ í‘œì‹œê°€ ì—†ìŠµë‹ˆë‹¤",
        "ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"
    ]
    
    has_critical_issue = any(
        any(critical in issue for critical in critical_issues)
        for issue in validation_result.issues
    )
    
    if has_critical_issue:
        logger.info("ì‹¬ê°í•œ êµ¬ì¡°ì  ë¬¸ì œ ë°œê²¬ - ì¬ì‹œë„ í•„ìš”")
        return True
    
    # ë¬¸ì œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¬ì‹œë„
    if len(validation_result.issues) >= 5:
        logger.info(f"ë¬¸ì œì  ê³¼ë‹¤ ({len(validation_result.issues)}ê°œ) - ì¬ì‹œë„ í•„ìš”")
        return True
    
    # ê·¸ ì™¸ì˜ ê²½ìš°ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ (ë¯¸ì„¸í•œ ë¬¸ì œëŠ” í—ˆìš©)
    logger.info("ë¯¸ì„¸í•œ ë¬¸ì œë§Œ ìˆìŒ - ì¬ì‹œë„ ë¶ˆí•„ìš”")
    return False

def get_retry_feedback(validation_result: ValidationResult) -> str:
    """
    ì¬ì‹œë„ë¥¼ ìœ„í•œ í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±
    
    ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ìƒì„± ì‹œ ì°¸ê³ í•  ìˆ˜ ìˆëŠ”
    êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.
    
    Args:
        validation_result: ê²€ì¦ ê²°ê³¼
        
    Returns:
        str: ì¬ì‹œë„ìš© í”¼ë“œë°± ë©”ì‹œì§€
    """
    feedback_parts = []
    
    # ì‹¬ê°í•œ ë¬¸ì œë¶€í„° ìš°ì„  ì²˜ë¦¬
    critical_issues = []
    minor_issues = []
    
    for issue in validation_result.issues:
        if any(critical in issue for critical in ["ì œëª©", "ì„¹ì…˜", "ì¶œì²˜ í‘œì‹œê°€ ì—†ìŠµë‹ˆë‹¤"]):
            critical_issues.append(issue)
        else:
            minor_issues.append(issue)
    
    if critical_issues:
        feedback_parts.append(f"í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­: {'; '.join(critical_issues[:2])}")
    
    if minor_issues:
        feedback_parts.append(f"ì¶”ê°€ ê°œì„ ì‚¬í•­: {'; '.join(minor_issues[:2])}")
    
    # êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ ì¶”ê°€
    if validation_result.suggestions:
        feedback_parts.append(f"ê°œì„  ë°©í–¥: {validation_result.suggestions[0]}")
    
    return " | ".join(feedback_parts) if feedback_parts else validation_result.feedback