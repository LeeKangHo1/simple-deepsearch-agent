# agents/question_analyzer.py
"""
ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸

ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¶„í•´í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
LLMì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , íš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ í•˜ìœ„ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ íŒŒì•… ë° í‚¤ì›Œë“œ ì¶”ì¶œ
- ë‹¤ê°ë„ ê´€ì ì—ì„œ í•˜ìœ„ ì§ˆë¬¸ ìƒì„± (3-5ê°œ)
- ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ í˜•íƒœë¡œ ë³€í™˜
- ì¤‘ë³µ ì¿¼ë¦¬ ì œê±° ë° í’ˆì§ˆ ê²€ì¦
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional
import time

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.state import ResearchState, StateManager
from services.llm_service import get_llm_service, LLMResponse
from utils.validators import validate_search_query, validate_query_list, sanitize_input
from utils.logger import get_agent_logger, log_agent_start, log_agent_end

logger = get_agent_logger("question_analyzer")


class QuestionAnalyzer:
    """
    ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ë°›ì•„ì„œ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ
    ì—¬ëŸ¬ ê°œì˜ í•˜ìœ„ ì¿¼ë¦¬ë¡œ ë¶„í•´í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    
    ì‚¬ìš© ì˜ˆì‹œ:
        analyzer = QuestionAnalyzer()
        result = await analyzer.analyze("ì˜¤í”ˆì†ŒìŠ¤ LLM íŠ¸ë Œë“œ ì•Œë ¤ì¤˜")
        # result: ["ì˜¤í”ˆì†ŒìŠ¤ LLM ìµœì‹  ë™í–¥", "LLaMA Mistral ì„±ëŠ¥ ë¹„êµ", ...]
    """
    
    def __init__(self, max_queries: int = 4, enable_keyword_extraction: bool = True):
        """
        ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            max_queries: ìƒì„±í•  ìµœëŒ€ ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ 4ê°œ)
            enable_keyword_extraction: í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        """
        self.llm_service = get_llm_service()
        self.max_queries = max_queries
        self.enable_keyword_extraction = enable_keyword_extraction
        
        # ì„±ëŠ¥ í†µê³„
        self.total_requests = 0
        self.total_queries_generated = 0
        self.avg_processing_time = 0.0
        
        logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ìµœëŒ€ ì¿¼ë¦¬: {max_queries}ê°œ)")
    
    async def analyze_question(self, user_input: str) -> List[str]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•˜ìœ„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        ì´ ë©”ì„œë“œëŠ” ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•œ ê³µê°œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
        LangGraph ì›Œí¬í”Œë¡œìš° ì™¸ë¶€ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        Args:
            user_input: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[str]: ìƒì„±ëœ í•˜ìœ„ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
            Exception: LLM í˜¸ì¶œ ì‹¤íŒ¨ ë“± ê¸°íƒ€ ì˜¤ë¥˜
        """
        log_agent_start("question_analyzer", user_input)
        start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: ì…ë ¥ ê²€ì¦ ë° ì •ì œ
            validated_input = self._validate_and_clean_input(user_input)
            logger.debug(f"ì…ë ¥ ê²€ì¦ ì™„ë£Œ: '{validated_input[:50]}...'")
            
            # 2ë‹¨ê³„: LLMì„ í†µí•œ í•˜ìœ„ ì¿¼ë¦¬ ìƒì„±
            queries = await self._generate_sub_queries(validated_input)
            logger.debug(f"LLM ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {len(queries)}ê°œ")
            
            # 3ë‹¨ê³„: ì¿¼ë¦¬ í›„ì²˜ë¦¬ (ê²€ì¦, ì¤‘ë³µ ì œê±°, í’ˆì§ˆ ê°œì„ )
            final_queries = self._post_process_queries(queries, validated_input)
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_statistics(len(final_queries), processing_time)
            
            log_agent_end("question_analyzer", success=True, output_data=final_queries)
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {len(final_queries)}ê°œ ì¿¼ë¦¬ ìƒì„± ({processing_time:.2f}ì´ˆ)")
            
            return final_queries
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_statistics(0, processing_time)
            
            log_agent_end("question_analyzer", success=False, error=str(e))
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨ ({processing_time:.2f}ì´ˆ): {e}")
            raise
    
    async def process_state(self, state: ResearchState) -> ResearchState:
        """
        LangGraph ì›Œí¬í”Œë¡œìš°ìš© ìƒíƒœ ì²˜ë¦¬ ë©”ì„œë“œ
        
        ResearchStateë¥¼ ì…ë ¥ë°›ì•„ ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ê°€í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
        ì´ ë©”ì„œë“œëŠ” LangGraph ë…¸ë“œì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
        
        Args:
            state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
            
        Returns:
            ResearchState: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        """
        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        new_state = StateManager.set_step(
            state, 
            "analyzing", 
            "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„± ì¤‘..."
        )
        
        try:
            user_input = state["user_input"]
            if not user_input:
                raise ValueError("ì‚¬ìš©ì ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ì§ˆë¬¸ ë¶„ì„ ìˆ˜í–‰
            sub_queries = await self.analyze_question(user_input)
            
            # ìƒíƒœì— ê²°ê³¼ ì €ì¥
            new_state = new_state.copy()
            new_state["sub_queries"] = sub_queries
            
            # ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            new_state = StateManager.add_log(
                new_state, 
                f"âœ… ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {len(sub_queries)}ê°œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"
            )
            
            logger.info(f"ìƒíƒœ ì²˜ë¦¬ ì™„ë£Œ: {len(sub_queries)}ê°œ ì¿¼ë¦¬ë¥¼ ìƒíƒœì— ì €ì¥")
            return new_state
            
        except Exception as e:
            # ì˜¤ë¥˜ ìƒíƒœ ì„¤ì •
            error_state = StateManager.set_error(new_state, f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒíƒœ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return error_state
    
    def _validate_and_clean_input(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ê²€ì¦í•˜ê³  ì •ì œí•©ë‹ˆë‹¤.
        
        Args:
            user_input: ì›ë³¸ ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            str: ê²€ì¦ë˜ê³  ì •ì œëœ ì…ë ¥
            
        Raises:
            ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        """
        if not user_input or not isinstance(user_input, str):
            raise ValueError("ì‚¬ìš©ì ì…ë ¥ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì…ë ¥ ì •ì œ (ë³´ì•ˆ ë° í’ˆì§ˆ í–¥ìƒ)
        cleaned_input = sanitize_input(user_input, max_length=1000)
        
        if len(cleaned_input.strip()) < 3:
            raise ValueError("ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 3ì ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        if len(cleaned_input) > 500:
            raise ValueError("ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 500ì ì´ë‚´ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        return cleaned_input
    
    async def _generate_sub_queries(self, question: str) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            question: ë¶„ì„í•  ì§ˆë¬¸
            
        Returns:
            List[str]: ìƒì„±ëœ í•˜ìœ„ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            Exception: LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        logger.debug(f"LLM í•˜ìœ„ ì¿¼ë¦¬ ìƒì„± ì‹œì‘: '{question[:50]}...'")
        
        try:
            # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì¿¼ë¦¬ ìƒì„±
            response: LLMResponse = await self.llm_service.generate_sub_queries(
                user_question=question,
                num_queries=self.max_queries
            )
            
            if not response.success:
                raise Exception(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {response.error_message}")
            
            # ì‘ë‹µ íŒŒì‹± (JSON ë°°ì—´ í˜•íƒœë¡œ ë°›ìŒ)
            try:
                queries = json.loads(response.content)
                if not isinstance(queries, list):
                    raise ValueError("LLM ì‘ë‹µì´ ë°°ì—´ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
                
                # ë¬¸ìì—´ì´ ì•„ë‹Œ í•­ëª© ì œê±°
                queries = [str(q).strip() for q in queries if q and str(q).strip()]
                
                logger.debug(f"LLM ì‘ë‹µ íŒŒì‹± ì™„ë£Œ: {len(queries)}ê°œ ì¿¼ë¦¬")
                return queries
                
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ì„ ì§ì ‘ ì²˜ë¦¬
                logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, ë¬¸ìì—´ ì§ì ‘ ì²˜ë¦¬ ì‹œë„")
                return self._parse_queries_from_text(response.content)
                
        except Exception as e:
            logger.error(f"í•˜ìœ„ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
            return self._generate_fallback_queries(question)
    
    def _parse_queries_from_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë°± ë©”ì„œë“œ
        
        LLMì´ JSON í˜•íƒœê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µí•œ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            text: íŒŒì‹±í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¶”ì¶œëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        queries = []
        
        # ì—¬ëŸ¬ ê°€ì§€ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹± ì‹œë„
        patterns = [
            r'"([^"]+)"',           # "ì¿¼ë¦¬" í˜•íƒœ
            r"'([^']+)'",           # 'ì¿¼ë¦¬' í˜•íƒœ
            r'^\d+\.\s*(.+)$',      # 1. ì¿¼ë¦¬ í˜•íƒœ
            r'^[-*]\s*(.+)$',       # - ì¿¼ë¦¬, * ì¿¼ë¦¬ í˜•íƒœ
        ]
        
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            for pattern in patterns:
                import re
                matches = re.findall(pattern, line, re.MULTILINE)
                for match in matches:
                    query = match.strip()
                    if len(query) > 5 and query not in queries:  # ì¤‘ë³µ ì œê±°
                        queries.append(query)
                        break
        
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ìˆœ ë¶„í• 
        if not queries:
            for line in lines:
                line = line.strip()
                if len(line) > 5:
                    queries.append(line)
        
        logger.debug(f"í…ìŠ¤íŠ¸ì—ì„œ {len(queries)}ê°œ ì¿¼ë¦¬ ì¶”ì¶œ")
        return queries[:self.max_queries]
    
    def _generate_fallback_queries(self, question: str) -> List[str]:
        """
        LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” í´ë°± ì¿¼ë¦¬ ìƒì„±
        
        í‚¤ì›Œë“œ ì¶”ì¶œê³¼ ê°„ë‹¨í•œ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ì ì¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[str]: í´ë°± ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.warning("í´ë°± ì¿¼ë¦¬ ìƒì„± ëª¨ë“œ ì‹¤í–‰")
        
        queries = []
        
        # ì›ë³¸ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì²« ë²ˆì§¸ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
        queries.append(question)
        
        if self.enable_keyword_extraction:
            try:
                from utils.text_processing import extract_keywords
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = extract_keywords(question, max_keywords=8)
                
                if keywords:
                    # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì¶”ê°€ ì¿¼ë¦¬ ìƒì„±
                    if len(keywords) >= 2:
                        queries.append(' '.join(keywords[:3]))  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
                    
                    if len(keywords) >= 4:
                        queries.append(' '.join(keywords[2:5]))  # ì¤‘ê°„ 3ê°œ í‚¤ì›Œë“œ
                    
                    # ê°œë³„ ì¤‘ìš” í‚¤ì›Œë“œë“¤
                    for keyword in keywords[:2]:
                        if len(keyword) > 2:
                            queries.append(keyword)
                
            except ImportError:
                logger.warning("í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
        if 'íŠ¸ë Œë“œ' in question or 'ë™í–¥' in question:
            base = question.replace('íŠ¸ë Œë“œ', '').replace('ë™í–¥', '').strip()
            queries.append(f"{base} ìµœì‹  ë™í–¥")
            queries.append(f"{base} ì‹œì¥ ì „ë§")
        
        if 'ë¹„êµ' in question or 'vs' in question.lower():
            queries.append(question.replace('ë¹„êµ', 'ì°¨ì´ì '))
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_queries = []
        for query in queries:
            if query not in unique_queries and len(query.strip()) > 3:
                unique_queries.append(query.strip())
        
        result = unique_queries[:self.max_queries]
        logger.info(f"í´ë°± ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {len(result)}ê°œ")
        
        return result
    
    def _post_process_queries(self, queries: List[str], original_question: str) -> List[str]:
        """
        ìƒì„±ëœ ì¿¼ë¦¬ë“¤ì„ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        ê²€ì¦, ì¤‘ë³µ ì œê±°, í’ˆì§ˆ ê°œì„  ë“±ì„ ìˆ˜í–‰í•˜ì—¬ ìµœì¢… ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            queries: ì›ë³¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            original_question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[str]: í›„ì²˜ë¦¬ëœ ìµœì¢… ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not queries:
            logger.warning("ìƒì„±ëœ ì¿¼ë¦¬ê°€ ì—†ìŒ, ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©")
            return [original_question]
        
        logger.debug(f"ì¿¼ë¦¬ í›„ì²˜ë¦¬ ì‹œì‘: {len(queries)}ê°œ â†’ ê²€ì¦ ë° ì •ì œ")
        
        # 1ë‹¨ê³„: ê²€ì¦ ë° ì¤‘ë³µ ì œê±°
        valid_queries, removed_count = validate_query_list(queries)
        
        if removed_count > 0:
            logger.debug(f"ê²€ì¦ ë‹¨ê³„ì—ì„œ {removed_count}ê°œ ì¿¼ë¦¬ ì œê±°ë¨")
        
        # 2ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í™•ì¸
        filtered_queries = self._filter_by_relevance(valid_queries, original_question)
        
        # 3ë‹¨ê³„: ê¸¸ì´ ë° í’ˆì§ˆ ì¡°ì •
        optimized_queries = self._optimize_query_quality(filtered_queries)
        
        # 4ë‹¨ê³„: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        final_queries = optimized_queries[:self.max_queries]
        
        # ìµœì†Œ 1ê°œëŠ” ë³´ì¥ (ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©)
        if not final_queries:
            final_queries = [original_question]
            logger.warning("í›„ì²˜ë¦¬ ê²°ê³¼ ì¿¼ë¦¬ê°€ ì—†ì–´ ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©")
        
        logger.debug(f"ì¿¼ë¦¬ í›„ì²˜ë¦¬ ì™„ë£Œ: {len(final_queries)}ê°œ ìµœì¢… ì¿¼ë¦¬")
        return final_queries
    
    def _filter_by_relevance(self, queries: List[str], original_question: str) -> List[str]:
        """
        ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            queries: í•„í„°ë§í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            original_question: ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[str]: ê´€ë ¨ì„±ì´ ë†’ì€ ì¿¼ë¦¬ë“¤
        """
        if not queries:
            return []
        
        try:
            from utils.text_processing import extract_keywords, calculate_text_similarity
            
            # ì›ë³¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            original_keywords = set(extract_keywords(original_question.lower(), max_keywords=10))
            
            relevant_queries = []
            for query in queries:
                # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± í™•ì¸
                query_keywords = set(extract_keywords(query.lower(), max_keywords=10))
                
                # ê³µí†µ í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë†’ì€ ê²½ìš° ìœ ì§€
                common_keywords = original_keywords & query_keywords
                similarity = calculate_text_similarity(original_question, query)
                
                if len(common_keywords) > 0 or similarity > 0.2:
                    relevant_queries.append(query)
                else:
                    logger.debug(f"ê´€ë ¨ì„± ë‚®ì€ ì¿¼ë¦¬ ì œê±°: '{query}'")
            
            return relevant_queries if relevant_queries else queries  # í´ë°±
            
        except Exception as e:
            logger.warning(f"ê´€ë ¨ì„± í•„í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€: {e}")
            return queries
    
    def _optimize_query_quality(self, queries: List[str]) -> List[str]:
        """
        ì¿¼ë¦¬ì˜ í’ˆì§ˆì„ ìµœì í™”í•©ë‹ˆë‹¤.
        
        ê²€ìƒ‰ì— ë” íš¨ê³¼ì ì¸ í˜•íƒœë¡œ ì¿¼ë¦¬ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
        
        Args:
            queries: ìµœì í™”í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ìµœì í™”ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        optimized = []
        
        for query in queries:
            # ê¸°ë³¸ ì •ì œ
            cleaned = query.strip()
            
            # ë„ˆë¬´ ê¸´ ì¿¼ë¦¬ ë‹¨ì¶•
            if len(cleaned) > 100:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
                sentences = cleaned.split('.')
                cleaned = sentences[0].strip()
                if len(cleaned) > 100:
                    cleaned = cleaned[:100].strip()
            
            # ê²€ìƒ‰ì— ë¶ˆí•„ìš”í•œ í‘œí˜„ ì œê±°
            remove_patterns = [
                r'ì•Œë ¤ì£¼ì„¸ìš”?\??',
                r'ê¶ê¸ˆí•©ë‹ˆë‹¤?\??',
                r'ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\??',
                r'ì„¤ëª…í•´ì£¼ì„¸ìš”?\??',
                r'ë¬´ì—‡ì¸ê°€ìš”?\??',
                r'ì°¾ì•„ì£¼ì„¸ìš”?\??',
            ]
            
            for pattern in remove_patterns:
                import re
                cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
            
            # ê²€ìƒ‰ íš¨ìœ¨ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì¡°ì •
            cleaned = cleaned.strip().rstrip('.,!?')
            
            if len(cleaned) > 5:
                optimized.append(cleaned)
        
        return optimized
    
    def _update_statistics(self, queries_generated: int, processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_requests += 1
        self.total_queries_generated += queries_generated
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        if self.total_requests == 1:
            self.avg_processing_time = processing_time
        else:
            self.avg_processing_time = (
                (self.avg_processing_time * (self.total_requests - 1) + processing_time) 
                / self.total_requests
            )
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì„±ëŠ¥ í†µê³„ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì„±ëŠ¥ í†µê³„ ì •ë³´
        """
        return {
            "total_requests": self.total_requests,
            "total_queries_generated": self.total_queries_generated,
            "avg_queries_per_request": (
                self.total_queries_generated / max(self.total_requests, 1)
            ),
            "avg_processing_time": round(self.avg_processing_time, 3),
            "max_queries": self.max_queries,
            "keyword_extraction_enabled": self.enable_keyword_extraction
        }
    
    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.total_requests = 0
        self.total_queries_generated = 0
        self.avg_processing_time = 0.0
        logger.info("ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ í†µê³„ ì´ˆê¸°í™”ë¨")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_question_analyzer_instance = None

def get_question_analyzer() -> QuestionAnalyzer:
    """
    ì „ì—­ ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        QuestionAnalyzer: ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    """
    global _question_analyzer_instance
    
    if _question_analyzer_instance is None:
        _question_analyzer_instance = QuestionAnalyzer()
        logger.info("ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨")
    
    return _question_analyzer_instance

def reset_question_analyzer():
    """ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ìš©)"""
    global _question_analyzer_instance
    _question_analyzer_instance = None
    logger.info("ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ë¨")


# LangGraph ë…¸ë“œ í•¨ìˆ˜ (ì›Œí¬í”Œë¡œìš°ì—ì„œ ì§ì ‘ ì‚¬ìš©)
async def analyze_question_node(state: ResearchState) -> ResearchState:
    """
    LangGraph ì›Œí¬í”Œë¡œìš°ìš© ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” LangGraphì—ì„œ ì§ì ‘ í˜¸ì¶œë˜ë©°, 
    ì „ì—­ QuestionAnalyzer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
        
    Returns:
        ResearchState: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
    """
    analyzer = get_question_analyzer()
    return await analyzer.process_state(state)


# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© í—¬í¼ í•¨ìˆ˜ë“¤
async def test_question_analyzer(test_question: str = None):
    """
    ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    
    Args:
        test_question: í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ (ê¸°ë³¸ê°’: ìƒ˜í”Œ ì§ˆë¬¸)
    """
    if test_question is None:
        test_question = "ì˜¤í”ˆì†ŒìŠ¤ LLM íŠ¸ë Œë“œì™€ ìƒì—…ìš© ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ì  ì•Œë ¤ì¤˜"
    
    print(f"ğŸ§ª ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_question}")
    print("-" * 50)
    
    try:
        analyzer = QuestionAnalyzer()
        
        start_time = time.time()
        queries = await analyzer.analyze_question(test_question)
        end_time = time.time()
        
        print(f"âœ… ì„±ê³µ! {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„± ({end_time - start_time:.2f}ì´ˆ)")
        print("\nğŸ“‹ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤:")
        for i, query in enumerate(queries, 1):
            print(f"  {i}. {query}")
        
        print(f"\nğŸ“Š ì—ì´ì „íŠ¸ í†µê³„:")
        stats = analyzer.get_statistics()
        for key, value in stats.items():
            print(f"  - {key}: {value}")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise





if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    asyncio.run(test_question_analyzer())