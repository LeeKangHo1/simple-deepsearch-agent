# agents/insight_generator.py
"""
ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸

ë¬¸ì„œ ìš”ì•½ë“¤ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ ê¹Šì´ ìˆëŠ” í†µì°°ê³¼ ì‹œì‚¬ì ì„ ë„ì¶œí•©ë‹ˆë‹¤.
ë‹¨ìˆœí•œ ì •ë³´ ë‚˜ì—´ì´ ì•„ë‹Œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´, íŠ¸ë Œë“œ, ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬
ì‚¬ìš©ìì—ê²Œ ê°€ì¹˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë‹¤ì¤‘ ë¬¸ì„œ ìš”ì•½ ì¢…í•© ë¶„ì„
- íŒ¨í„´ ë° íŠ¸ë Œë“œ ë„ì¶œ
- ì›ì¸-ê²°ê³¼ ê´€ê³„ ë¶„ì„
- ë¯¸ë˜ ì „ë§ ë° ì‹œì‚¬ì  ìƒì„±
- ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì²˜ë¦¬
"""

import json
import re
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import asdict

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.data_models import Insight, DocumentSummary
from models.state import ResearchState, StateManager
from services.llm_service import get_llm_service, LLMResponse
from utils.text_processing import clean_text, is_valid_text, extract_keywords
from utils.validators import validate_insights
from utils.logger import create_agent_logger, log_execution_time

logger = logging.getLogger(__name__)
agent_logger = create_agent_logger("insight_generator")

class InsightGenerator:
    """
    ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    ì—¬ëŸ¬ ë¬¸ì„œ ìš”ì•½ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” í†µì°°ì„ ë„ì¶œí•©ë‹ˆë‹¤.
    ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ì²˜ë¦¬ì™€ êµ¬ë¶„ì ê¸°ë°˜ íŒŒì‹±ì„ ì§€ì›í•©ë‹ˆë‹¤.
    
    ì²˜ë¦¬ íë¦„:
    1. ë¬¸ì„œ ìš”ì•½ ì „ì²˜ë¦¬ ë° ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
    2. LLMì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
    3. JSON/êµ¬ë¶„ì ê¸°ë°˜ íŒŒì‹±
    4. Insight ê°ì²´ ìƒì„± ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    5. í’ˆì§ˆ ê²€ì¦ ë° í›„ì²˜ë¦¬
    """
    
    def __init__(self, min_insight_length: int = 30, target_insight_count: int = 4):
        """
        ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            min_insight_length: ì¸ì‚¬ì´íŠ¸ ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸: 30ì)
            target_insight_count: ëª©í‘œ ì¸ì‚¬ì´íŠ¸ ê°œìˆ˜ (ê¸°ë³¸: 4ê°œ)
        """
        self.min_insight_length = min_insight_length
        self.target_insight_count = target_insight_count
        self.llm_service = get_llm_service()
        
        # í†µê³„ ì¶”ì 
        self.total_summaries_processed = 0
        self.total_insights_generated = 0
        self.json_parse_failures = 0
        self.separator_parse_count = 0
    
    @log_execution_time
    async def generate_insights(
        self, 
        summaries: List[DocumentSummary], 
        user_question: str = ""
    ) -> List[Insight]:
        """
        ë¬¸ì„œ ìš”ì•½ë“¤ë¡œë¶€í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±
        
        Args:
            summaries: DocumentSummary ê°ì²´ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[Insight]: ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if not summaries:
            logger.warning("ì¸ì‚¬ì´íŠ¸ ìƒì„±í•  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        agent_logger.start_step(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘ ({len(summaries)}ê°œ ìš”ì•½)")
        
        try:
            # 1ë‹¨ê³„: ìš”ì•½ ì „ì²˜ë¦¬ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
            processed_summaries = self._preprocess_summaries(summaries)
            
            if not processed_summaries:
                agent_logger.end_step("ìš”ì•½ ì „ì²˜ë¦¬", False, "ìœ íš¨í•œ ìš”ì•½ì´ ì—†ìŒ")
                return []
            
            # 2ë‹¨ê³„: LLMì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            llm_response = await self._generate_insights_llm(processed_summaries, user_question)
            
            if not llm_response.success:
                agent_logger.end_step("ì¸ì‚¬ì´íŠ¸ ìƒì„±", False, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {llm_response.error_message}")
                return []
            
            # 3ë‹¨ê³„: ì‘ë‹µ íŒŒì‹± ë° Insight ê°ì²´ ìƒì„±
            insights = self._parse_insights_response(llm_response.content, summaries, user_question)
            
            # 4ë‹¨ê³„: í†µê³„ ì—…ë°ì´íŠ¸
            self.total_summaries_processed += len(summaries)
            self.total_insights_generated += len(insights)
            
            agent_logger.end_step(
                "ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ",
                True,
                f"{len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"
            )
            
            return insights
            
        except Exception as e:
            agent_logger.end_step("ì¸ì‚¬ì´íŠ¸ ìƒì„±", False, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def _preprocess_summaries(self, summaries: List[DocumentSummary]) -> List[Dict[str, Any]]:
        """
        ìš”ì•½ ì „ì²˜ë¦¬ ë° ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Args:
            summaries: DocumentSummary ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict[str, Any]]: ì „ì²˜ë¦¬ëœ ìš”ì•½ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        processed = []
        
        for i, summary in enumerate(summaries):
            try:
                # ìš”ì•½ ìœ íš¨ì„± ê²€ì¦
                if not summary.summary or len(summary.summary.strip()) < 10:
                    logger.debug(f"ìš”ì•½ {i+1} ì œì™¸: ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ")
                    continue
                
                # ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
                confidence = max(0.1, summary.confidence_score)  # ìµœì†Œ 0.1 ë³´ì¥
                weight = confidence  # ì‹ ë¢°ë„ê°€ ê³§ ê°€ì¤‘ì¹˜
                
                processed_summary = {
                    "index": i + 1,
                    "content": summary.summary.strip(),
                    "confidence_score": confidence,
                    "weight": weight,
                    "document_hash": summary.document_hash,
                    "key_points": summary.key_points,
                    "word_count": summary.word_count
                }
                
                processed.append(processed_summary)
                
            except Exception as e:
                logger.warning(f"ìš”ì•½ {i+1} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì‹ ë¢°ë„ ìš°ì„ )
        processed.sort(key=lambda x: x["weight"], reverse=True)
        
        logger.info(f"ìš”ì•½ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed)}/{len(summaries)}ê°œ ìœ íš¨")
        return processed
    
    async def _generate_insights_llm(
        self, 
        processed_summaries: List[Dict[str, Any]], 
        user_question: str
    ) -> LLMResponse:
        """
        LLMì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        
        Args:
            processed_summaries: ì „ì²˜ë¦¬ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            LLMResponse: LLM ì‘ë‹µ
        """
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ë ¤í•œ ìš”ì•½ í…ìŠ¤íŠ¸ êµ¬ì„±
        weighted_summaries = []
        for summary in processed_summaries:
            weight_indicator = "â­" * min(5, int(summary["weight"] * 5))  # ì‹ ë¢°ë„ ì‹œê°í™”
            weighted_text = f"ë¬¸ì„œ {summary['index']} {weight_indicator} (ì‹ ë¢°ë„: {summary['confidence_score']:.2f}): {summary['content']}"
            weighted_summaries.append(weighted_text)
        
        summaries_text = "\n\n".join(weighted_summaries)
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ ê°•í™”
        question_context = ""
        if user_question.strip():
            question_context = f"\n\n**ì¤‘ìš”**: íŠ¹íˆ '{user_question}'ì™€ ê´€ë ¨ëœ í†µì°°ì— ìš°ì„ ìˆœìœ„ë¥¼ ë‘ì„¸ìš”."
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", f"""ë‹¹ì‹ ì€ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë¬¸ì„œ ìš”ì•½ì„ ì¢…í•©í•˜ì—¬ ê¹Šì´ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”.

ì¸ì‚¬ì´íŠ¸ ìƒì„± ê·œì¹™:
1. ë‹¨ìˆœí•œ ì •ë³´ ë‚˜ì—´ì´ ì•„ë‹Œ ì˜ë¯¸ ìˆëŠ” í†µì°° ì œê³µ
2. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê³µí†µ íŒ¨í„´ì´ë‚˜ íŠ¸ë Œë“œ íŒŒì•…
3. ì›ì¸ê³¼ ê²°ê³¼, ìƒê´€ê´€ê³„ ë“±ì„ ë¶„ì„
4. ë¯¸ë˜ ì „ë§ì´ë‚˜ ì‹œì‚¬ì  í¬í•¨
5. {self.target_insight_count}ê°œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±
6. ê° ì¸ì‚¬ì´íŠ¸ëŠ” ìµœì†Œ {self.min_insight_length}ì ì´ìƒ
7. ì‹ ë¢°ë„ ë†’ì€ ë¬¸ì„œ(â­ ë§ì€)ì˜ ë‚´ìš©ì— ë” ì¤‘ì ì„ ë‘˜ ê²ƒ{question_context}

ì¶œë ¥ í˜•ì‹ - ê° ì¸ì‚¬ì´íŠ¸ë¥¼ êµ¬ë¶„ìë¡œ ë¶„ë¦¬:
===INSIGHT_1===
ì²« ë²ˆì§¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸: êµ¬ì²´ì ì´ê³  ê¹Šì´ ìˆëŠ” í†µì°° ë‚´ìš©
===INSIGHT_2===
ë‘ ë²ˆì§¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸: íŒ¨í„´ì´ë‚˜ íŠ¸ë Œë“œ ë¶„ì„
===INSIGHT_3===
ì„¸ ë²ˆì§¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸: ì›ì¸ê³¼ ê²°ê³¼ ê´€ê³„ ë¶„ì„
===INSIGHT_4===
ë„¤ ë²ˆì§¸ í•µì‹¬ ì¸ì‚¬ì´íŠ¸: ë¯¸ë˜ ì „ë§ì´ë‚˜ ì‹œì‚¬ì 

ì¸ì‚¬ì´íŠ¸ ì˜ˆì‹œ:
- "ê¸°ì—…ë“¤ì´ ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ì±„íƒí•˜ëŠ” ì´ìœ ëŠ” ë¹„ìš© ì ˆê°ë³´ë‹¤ ìœ ì—°ì„±ê³¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥ì„± ë•Œë¬¸ì´ë©°, ì´ëŠ” ê¸°ìˆ  ìë¦½ë„ë¥¼ ë†’ì´ë ¤ëŠ” ì „ëµì  íŒë‹¨ìœ¼ë¡œ ë³´ì¸ë‹¤"
- "ìƒì—…ìš© ëª¨ë¸ê³¼ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ ì„±ëŠ¥ ê²©ì°¨ê°€ ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë©´ì„œ ì‹œì¥ íŒë„ê°€ ë³€í™”í•˜ê³  ìˆìœ¼ë©°, í–¥í›„ 2-3ë…„ ë‚´ ì˜¤í”ˆì†ŒìŠ¤ê°€ ì£¼ë¥˜ê°€ ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤" """),
            ("user", "ì›ë³¸ ì§ˆë¬¸: {question}\n\në¬¸ì„œ ìš”ì•½ë“¤ (ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©):\n{summaries}")
        ])
        
        return await self.llm_service._call_llm(
            prompt_template=prompt_template,
            input_variables={"question": user_question, "summaries": summaries_text},
            agent_type="insight_generator",
            output_parser=self.llm_service.str_parser,
            expected_type=str
        )
    
    def _parse_insights_response(
        self, 
        response_content: str, 
        original_summaries: List[DocumentSummary],
        user_question: str
    ) -> List[Insight]:
        """
        LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ Insight ê°ì²´ ìƒì„±
        
        Args:
            response_content: LLM ì‘ë‹µ ë‚´ìš©
            original_summaries: ì›ë³¸ DocumentSummary ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            List[Insight]: íŒŒì‹±ëœ ì¸ì‚¬ì´íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        insights = []
        
        try:
            # 1ë‹¨ê³„: JSON íŒŒì‹± ì‹œë„
            insight_texts = self._try_json_parsing(response_content)
            
            # 2ë‹¨ê³„: JSON ì‹¤íŒ¨ ì‹œ êµ¬ë¶„ì ê¸°ë°˜ íŒŒì‹± (í´ë°±)
            if not insight_texts:
                self.json_parse_failures += 1
                insight_texts = self._parse_with_separators(response_content)
                self.separator_parse_count += 1
                logger.debug("êµ¬ë¶„ì ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ í´ë°±")
            
            # 3ë‹¨ê³„: Insight ê°ì²´ ìƒì„±
            for i, insight_text in enumerate(insight_texts):
                insight_text = insight_text.strip()
                
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦
                if len(insight_text) < self.min_insight_length:
                    logger.debug(f"ì¸ì‚¬ì´íŠ¸ {i+1} ì œì™¸: ë„ˆë¬´ ì§§ìŒ ({len(insight_text)}ì)")
                    continue
                
                # ì¸ì‚¬ì´íŠ¸ ê°ì²´ ìƒì„±
                insight = self._create_insight_object(
                    content=insight_text,
                    index=i,
                    original_summaries=original_summaries,
                    user_question=user_question
                )
                
                insights.append(insight)
            
            logger.info(f"ì¸ì‚¬ì´íŠ¸ íŒŒì‹± ì™„ë£Œ: {len(insights)}ê°œ ìƒì„±")
            return insights
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _try_json_parsing(self, content: str) -> List[str]:
        """
        JSON íŒŒì‹± ì‹œë„
        
        Args:
            content: íŒŒì‹±í•  ë‚´ìš©
            
        Returns:
            List[str]: íŒŒì‹±ëœ ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        """
        try:
            # JSON ë°°ì—´ íŒ¨í„´ ì°¾ê¸°
            json_pattern = r'\[.*?\]'
            json_matches = re.findall(json_pattern, content, re.DOTALL)
            
            for match in json_matches:
                try:
                    parsed = json.loads(match)
                    if isinstance(parsed, list) and len(parsed) > 0:
                        return [str(item) for item in parsed if str(item).strip()]
                except json.JSONDecodeError:
                    continue
            
            # ì§ì ‘ JSON íŒŒì‹± ì‹œë„
            try:
                parsed = json.loads(content)
                if isinstance(parsed, list):
                    return [str(item) for item in parsed if str(item).strip()]
            except json.JSONDecodeError:
                pass
            
            return []
            
        except Exception as e:
            logger.debug(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_with_separators(self, content: str) -> List[str]:
        """
        êµ¬ë¶„ìë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ íŒŒì‹± (í´ë°± ì „ëµ)
        
        Args:
            content: íŒŒì‹±í•  ì‘ë‹µ ë‚´ìš©
            
        Returns:
            List[str]: ë¶„ë¦¬ëœ ì¸ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        insights = []
        
        try:
            # ===INSIGHT_N=== íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
            pattern = r'===INSIGHT_\d+===(.*?)(?====INSIGHT_\d+===|$)'
            matches = re.findall(pattern, content, re.DOTALL)
            
            if matches:
                insights = [match.strip() for match in matches if match.strip()]
                logger.debug(f"===INSIGHT_N=== íŒ¨í„´ íŒŒì‹±: {len(insights)}ê°œ")
            else:
                # ëŒ€ì•ˆ êµ¬ë¶„ì ì‹œë„: --- ë˜ëŠ” ===
                separators = ['---', '===', '***']
                for sep in separators:
                    parts = content.split(sep)
                    if len(parts) > 1:
                        insights = [part.strip() for part in parts if part.strip() and len(part.strip()) > 20]
                        if insights:
                            logger.debug(f"'{sep}' êµ¬ë¶„ì íŒŒì‹±: {len(insights)}ê°œ")
                            break
            
            # ë§ˆì§€ë§‰ í´ë°±: ì¤„ë°”ê¿ˆ ê¸°ë°˜ ë¶„ë¦¬
            if not insights:
                lines = content.split('\n')
                for line in lines:
                    line = line.strip()
                    # ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì°¾ê¸°
                    if (re.match(r'^\d+\.', line) or 
                        re.match(r'^[-*â€¢]', line) or 
                        len(line) > self.min_insight_length):
                        # ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì œê±°
                        cleaned = re.sub(r'^[\d\.\-\*â€¢\s]+', '', line).strip()
                        if len(cleaned) >= self.min_insight_length:
                            insights.append(cleaned)
                
                if insights:
                    logger.debug(f"ì¤„ë°”ê¿ˆ ê¸°ë°˜ íŒŒì‹±: {len(insights)}ê°œ")
            
            return insights[:self.target_insight_count * 2]  # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            
        except Exception as e:
            logger.error(f"êµ¬ë¶„ì íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _create_insight_object(
        self, 
        content: str, 
        index: int,
        original_summaries: List[DocumentSummary],
        user_question: str
    ) -> Insight:
        """
        Insight ê°ì²´ ìƒì„±
        
        Args:
            content: ì¸ì‚¬ì´íŠ¸ ë‚´ìš©
            index: ì¸ì‚¬ì´íŠ¸ ì¸ë±ìŠ¤
            original_summaries: ì›ë³¸ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            Insight: ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ê°ì²´
        """
        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
        category = self._classify_insight_category(content, user_question)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (ìš”ì•½ë“¤ì˜ í‰ê·  ì‹ ë¢°ë„ ê¸°ë°˜)
        confidence_score = self._calculate_insight_confidence(content, original_summaries)
        
        # ë’·ë°›ì¹¨í•˜ëŠ” ë¬¸ì„œ í•´ì‹œ ìˆ˜ì§‘
        supporting_documents = [summary.document_hash for summary in original_summaries]
        
        return Insight(
            content=content,
            category=category,
            confidence_score=confidence_score,
            supporting_documents=supporting_documents,
            created_at=datetime.now()
        )
    
    def _classify_insight_category(self, content: str, user_question: str) -> str:
        """
        ì¸ì‚¬ì´íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        
        Args:
            content: ì¸ì‚¬ì´íŠ¸ ë‚´ìš©
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬
        """
        content_lower = content.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        trend_keywords = ['íŠ¸ë Œë“œ', 'ì¦ê°€', 'ê°ì†Œ', 'ë³€í™”', 'ì„±ì¥', 'í™•ëŒ€', 'ì¶•ì†Œ', 'ê¸‰ì¦', 'ê¸‰ê°']
        analysis_keywords = ['ë¶„ì„', 'ì›ì¸', 'ì´ìœ ', 'ë•Œë¬¸', 'ê²°ê³¼', 'ì˜í–¥', 'ìƒê´€ê´€ê³„', 'ê´€ê³„']
        prediction_keywords = ['ì „ë§', 'ì˜ˆìƒ', 'ì˜ˆì¸¡', 'ë¯¸ë˜', 'í–¥í›„', 'ë  ê²ƒ', 'í•  ê²ƒ', 'ê°€ëŠ¥ì„±']
        recommendation_keywords = ['ê¶Œì¥', 'ì¶”ì²œ', 'ì œì•ˆ', 'í•´ì•¼', 'í•„ìš”', 'ì¤‘ìš”', 'ê³ ë ¤']
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        scores = {
            'trend': sum(1 for keyword in trend_keywords if keyword in content_lower),
            'analysis': sum(1 for keyword in analysis_keywords if keyword in content_lower),
            'prediction': sum(1 for keyword in prediction_keywords if keyword in content_lower),
            'recommendation': sum(1 for keyword in recommendation_keywords if keyword in content_lower)
        }
        
        # ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        else:
            return "general"
    
    def _calculate_insight_confidence(
        self, 
        insight_content: str, 
        summaries: List[DocumentSummary]
    ) -> float:
        """
        ì¸ì‚¬ì´íŠ¸ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            insight_content: ì¸ì‚¬ì´íŠ¸ ë‚´ìš©
            summaries: ì›ë³¸ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not summaries:
            return 0.5
        
        # ìš”ì•½ë“¤ì˜ í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(s.confidence_score for s in summaries) / len(summaries)
        
        # ì¸ì‚¬ì´íŠ¸ ê¸¸ì´ ë³´ë„ˆìŠ¤ (ë” ê¸´ ì¸ì‚¬ì´íŠ¸ê°€ ë” ì‹ ë¢°ë„ ë†’ìŒ)
        length_bonus = min(0.2, len(insight_content) / 500)  # ìµœëŒ€ 0.2 ë³´ë„ˆìŠ¤
        
        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ (ë” ë‹¤ì–‘í•œ í‚¤ì›Œë“œê°€ ë” ì‹ ë¢°ë„ ë†’ìŒ)
        keywords = extract_keywords(insight_content, max_keywords=10)
        diversity_bonus = min(0.1, len(keywords) / 20)  # ìµœëŒ€ 0.1 ë³´ë„ˆìŠ¤
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        final_confidence = avg_confidence + length_bonus + diversity_bonus
        
        return min(1.0, max(0.1, final_confidence))  # 0.1 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œ
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """
        ì²˜ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        return {
            "total_summaries_processed": self.total_summaries_processed,
            "total_insights_generated": self.total_insights_generated,
            "json_parse_failures": self.json_parse_failures,
            "separator_parse_count": self.separator_parse_count,
            "success_rate_percent": round(
                (self.total_insights_generated / max(1, self.total_summaries_processed)) * 100, 2
            ),
            "min_insight_length": self.min_insight_length,
            "target_insight_count": self.target_insight_count
        }


# LangGraph ë…¸ë“œ í•¨ìˆ˜
async def generate_insights_node(state: ResearchState) -> ResearchState:
    """
    ì¸ì‚¬ì´íŠ¸ ìƒì„± LangGraph ë…¸ë“œ
    
    Stateì˜ summariesë¥¼ ë°›ì•„ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ê³  insightsì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
        
    Returns:
        ResearchState: ì¸ì‚¬ì´íŠ¸ê°€ ì¶”ê°€ëœ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    logger.info("=== ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œ ì‹œì‘ ===")
    
    try:
        # í˜„ì¬ ë‹¨ê³„ ì„¤ì •
        state = StateManager.set_step(state, "generating_insights", "ğŸ§  ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
        
        # ìš”ì•½ í™•ì¸
        if not state.get("summaries"):
            error_msg = "ì¸ì‚¬ì´íŠ¸ ìƒì„±í•  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤."
            logger.warning(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # DocumentSummary ê°ì²´ ìƒì„± (ìš”ì•½ì´ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ë˜ì–´ ìˆëŠ” ê²½ìš°)
        summaries = []
        for i, summary_text in enumerate(state["summaries"]):
            try:
                # ê°„ë‹¨í•œ DocumentSummary ê°ì²´ ìƒì„±
                summary = DocumentSummary(
                    document_hash=f"summary_{i}",
                    summary=summary_text,
                    confidence_score=0.7,  # ê¸°ë³¸ ì‹ ë¢°ë„
                    word_count=len(summary_text)
                )
                summaries.append(summary)
            except Exception as e:
                logger.warning(f"DocumentSummary ìƒì„± ì‹¤íŒ¨: {e}")
                continue
        
        if not summaries:
            error_msg = "ìœ íš¨í•œ DocumentSummary ê°ì²´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸° ìƒì„± ë° ì‹¤í–‰
        generator = InsightGenerator()
        insights = await generator.generate_insights(
            summaries=summaries,
            user_question=state.get("user_input", "")
        )
        
        if not insights:
            error_msg = "ì¸ì‚¬ì´íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # ì¸ì‚¬ì´íŠ¸ë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ìƒíƒœì— ì €ì¥
        insight_texts = [insight.content for insight in insights]
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        new_state = state.copy()
        new_state["insights"] = insight_texts
        
        # ì²˜ë¦¬ í†µê³„ ë¡œê¹…
        stats = generator.get_processing_stats()
        logger.info(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {stats}")
        
        new_state = StateManager.add_log(
            new_state,
            f"ğŸ§  ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {len(insight_texts)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"
        )
        
        # ì¸ì‚¬ì´íŠ¸ ë‚´ìš© ìš”ì•½ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        for i, insight in enumerate(insights, 1):
            logger.debug(f"ì¸ì‚¬ì´íŠ¸ {i} ({insight.category}): {insight.content[:100]}...")
        
        logger.info("=== ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œ ì™„ë£Œ ===")
        return new_state
        
    except Exception as e:
        error_msg = f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return StateManager.set_error(state, error_msg)


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_insight_generator(target_count: int = 4) -> InsightGenerator:
    """
    ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        target_count: ëª©í‘œ ì¸ì‚¬ì´íŠ¸ ê°œìˆ˜
        
    Returns:
        InsightGenerator: ì„¤ì •ëœ ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    return InsightGenerator(target_insight_count=target_count)

