# agents/response_generator.py
"""
ì‘ë‹µ ìƒì„± ì—ì´ì „íŠ¸

ì¸ì‚¬ì´íŠ¸ì™€ ë¬¸ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìµœì¢… ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
êµ¬ì¡°í™”ëœ í˜•ì‹ê³¼ ì ì ˆí•œ ì¶œì²˜ í‘œì‹œë¥¼ í†µí•´ ì‹ ë¢°ë„ ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
- ì„¹ì…˜ë³„ ì¶œì²˜ í‘œì‹œ (í˜„í™© 30%, ì¸ì‚¬ì´íŠ¸ 50%, ì „ë§ 20%)
- ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ì œëª© ìƒì„±
- ëª¨ë“  ì¸ì‚¬ì´íŠ¸ í¬í•¨ ë³´ì¥
- ë¬¸ì„œ ì¶œì²˜ ìë™ ì¶”ì¶œ ë° ì •ë¦¬
"""

import re
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urlparse
from datetime import datetime

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.data_models import Document, Insight, ResearchResponse
from models.state import ResearchState, StateManager
from services.llm_service import get_llm_service, LLMResponse
from utils.text_processing import clean_text, extract_keywords, truncate_text
from utils.validators import validate_response_structure
from utils.logger import create_agent_logger, log_execution_time

logger = logging.getLogger(__name__)
agent_logger = create_agent_logger("response_generator")

class ResponseGenerator:
    """
    ì‘ë‹µ ìƒì„± ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
    
    ì¸ì‚¬ì´íŠ¸, ë¬¸ì„œ ìš”ì•½, ì¶œì²˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì‚¬ìš©ì ì§ˆë¬¸ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì •ë³´ë¥¼ êµ¬ì„±í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ì‘ë‹µ êµ¬ì¡°:
    - ì œëª©: ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ìë™ ìƒì„±
    - ì£¼ìš” í˜„í™© (30%): í•µì‹¬ ë‚´ìš©ê³¼ ì¶œì²˜
    - ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (50%): ëª¨ë“  ì¸ì‚¬ì´íŠ¸ í¬í•¨
    - í–¥í›„ ì „ë§ (20%): ì˜ˆìƒ ë³€í™”ë‚˜ íŠ¸ë Œë“œ
    - ì¶œì²˜ ëª©ë¡: ì°¸ê³ í•œ ëª¨ë“  ë¬¸ì„œ
    """
    
    def __init__(self, max_response_length: int = 8000):
        """
        ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            max_response_length: ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ (ê¸°ë³¸: 8000ì)
        """
        self.max_response_length = max_response_length
        self.llm_service = get_llm_service()
        
        # ì„¹ì…˜ë³„ ë¹„ì¤‘ ì„¤ì •
        self.section_ratios = {
            "í˜„í™©": 0.30,    # 30%
            "ì¸ì‚¬ì´íŠ¸": 0.50,  # 50%
            "ì „ë§": 0.20     # 20%
        }
        
        # í†µê³„ ì¶”ì 
        self.total_responses_generated = 0
        self.total_insights_included = 0
        self.total_sources_processed = 0
    
    @log_execution_time
    async def generate_response(
        self,
        insights: List[str],
        summaries: List[str],
        documents: List[Dict[str, Any]],
        user_question: str = ""
    ) -> ResearchResponse:
        """
        ìµœì¢… ë§ˆí¬ë‹¤ìš´ ì‘ë‹µ ìƒì„±
        
        Args:
            insights: ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            summaries: ë¬¸ì„œ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            documents: ì›ë³¸ ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ResearchResponse: ìƒì„±ëœ ì‘ë‹µ ê°ì²´
        """
        if not insights:
            logger.warning("ì‘ë‹µ ìƒì„±í•  ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self._create_empty_response()
        
        agent_logger.start_step(f"ì‘ë‹µ ìƒì„± ì‹œì‘ ({len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸)")
        
        try:
            # 1ë‹¨ê³„: ì¶œì²˜ ì •ë³´ ì²˜ë¦¬
            sources_info = self._process_sources(documents)
            
            # 2ë‹¨ê³„: LLMì„ í†µí•œ ì‘ë‹µ ìƒì„±
            llm_response = await self._generate_markdown_response(
                insights, summaries, sources_info, user_question
            )
            
            if not llm_response.success:
                agent_logger.end_step("ì‘ë‹µ ìƒì„±", False, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {llm_response.error_message}")
                return self._create_empty_response()
            
            # 3ë‹¨ê³„: ì‘ë‹µ í›„ì²˜ë¦¬ ë° ê²€ì¦
            final_response = self._post_process_response(
                llm_response.content, sources_info, insights
            )
            
            # 4ë‹¨ê³„: ResearchResponse ê°ì²´ ìƒì„±
            research_response = ResearchResponse(
                markdown_content=final_response,
                sources=sources_info,
                insights_count=len(insights),
                documents_used=len(documents),
                word_count=len(final_response)
            )
            
            # 5ë‹¨ê³„: í†µê³„ ì—…ë°ì´íŠ¸
            self.total_responses_generated += 1
            self.total_insights_included += len(insights)
            self.total_sources_processed += len(sources_info)
            
            agent_logger.end_step(
                "ì‘ë‹µ ìƒì„± ì™„ë£Œ",
                True,
                f"ì‘ë‹µ ê¸¸ì´: {len(final_response)}ì, ì¶œì²˜: {len(sources_info)}ê°œ"
            )
            
            return research_response
            
        except Exception as e:
            agent_logger.end_step("ì‘ë‹µ ìƒì„±", False, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._create_empty_response()
    
    def _process_sources(self, documents: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        ë¬¸ì„œë“¤ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ ë° ì •ë¦¬
        
        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict[str, str]]: ì •ë¦¬ëœ ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        sources = []
        seen_domains = set()
        
        for doc in documents:
            try:
                title = doc.get("title", "").strip()
                url = doc.get("url", "").strip()
                
                if not title or not url:
                    continue
                
                # ë„ë©”ì¸ ì¶”ì¶œ
                domain = self._extract_domain(url)
                
                # ì¤‘ë³µ ë„ë©”ì¸ ì²´í¬ (ê°™ì€ ì‚¬ì´íŠ¸ì—ì„œ ì—¬ëŸ¬ ë¬¸ì„œê°€ ì˜¨ ê²½ìš° ëŒ€í‘œ 1ê°œë§Œ)
                if domain in seen_domains:
                    continue
                
                seen_domains.add(domain)
                
                # ì œëª© ê¸¸ì´ ì¡°ì •
                display_title = truncate_text(title, 60, "...")
                
                source_info = {
                    "title": display_title,
                    "url": url,
                    "domain": domain
                }
                
                sources.append(source_info)
                
            except Exception as e:
                logger.warning(f"ì¶œì²˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ì¶œì²˜ ì²˜ë¦¬ ì™„ë£Œ: {len(sources)}ê°œ ê³ ìœ  ì¶œì²˜")
        return sources
    
    def _extract_domain(self, url: str) -> str:
        """
        URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        
        Args:
            url: ì¶”ì¶œí•  URL
            
        Returns:
            str: ë„ë©”ì¸ëª…
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # www. ì œê±°
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # í¬íŠ¸ ë²ˆí˜¸ ì œê±°
            if ':' in domain:
                domain = domain.split(':')[0]
            
            return domain if domain else 'unknown'
        except Exception:
            return 'unknown'
    
    async def _generate_markdown_response(
        self,
        insights: List[str],
        summaries: List[str],
        sources_info: List[Dict[str, str]],
        user_question: str
    ) -> LLMResponse:
        """
        LLMì„ í†µí•œ ë§ˆí¬ë‹¤ìš´ ì‘ë‹µ ìƒì„±
        
        Args:
            insights: ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            summaries: ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            sources_info: ì¶œì²˜ ì •ë³´
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            LLMResponse: LLM ì‘ë‹µ
        """
        # ì¸ì‚¬ì´íŠ¸ë¥¼ ëª¨ë‘ í¬í•¨í•œ í…ìŠ¤íŠ¸ ìƒì„±
        all_insights_text = "\n".join([f"- {insight}" for insight in insights])
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summaries_text = "\n".join([f"â€¢ {summary}" for summary in summaries])
        
        # ì¶œì²˜ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        source_domains = [source["domain"] for source in sources_info]
        sources_text = ", ".join(source_domains[:10])  # ìµœëŒ€ 10ê°œê¹Œì§€
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ì œëª© íŒíŠ¸
        title_hint = user_question if user_question.strip() else "ì—°êµ¬ ê²°ê³¼"
        
        # ì‘ë‹µ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", f"""ë‹¹ì‹ ì€ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‘ì„± ê·œì¹™:
1. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™” (ì œëª©, ì†Œì œëª©, ë¶ˆë¦¿ í¬ì¸íŠ¸ ë“±)
2. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì œëª© ìƒì„±
3. ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¥¼ ê²ƒ:
   - # [ì œëª©] (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜)
   - ## ì£¼ìš” í˜„í™© *(ì¶œì²˜: ë„ë©”ì¸ë“¤)*
   - ## ì£¼ìš” ì¸ì‚¬ì´íŠ¸ *(ì¶œì²˜: ë„ë©”ì¸ë“¤)*  
   - ## í–¥í›„ ì „ë§ *(ì¶œì²˜: ë„ë©”ì¸ë“¤)*
   - --- ì¶œì²˜ ëª©ë¡
4. ì„¹ì…˜ë³„ ë¹„ì¤‘: í˜„í™© 30%, ì¸ì‚¬ì´íŠ¸ 50%, ì „ë§ 20%
5. ëª¨ë“  ì¸ì‚¬ì´íŠ¸ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•¨
6. ê° ì„¹ì…˜ ì œëª© ì˜†ì— ì¶œì²˜ í‘œì‹œ: *(ì¶œì²˜: example.com, news.com)*
7. ìµœëŒ€ {self.max_response_length}ì ì´ë‚´
8. ê°ê´€ì ì´ê³  ì •í™•í•œ ì •ë³´ë§Œ í¬í•¨

**ì¤‘ìš”**: ì œê³µëœ ëª¨ë“  ì¸ì‚¬ì´íŠ¸ë¥¼ ë°˜ë“œì‹œ 'ì£¼ìš” ì¸ì‚¬ì´íŠ¸' ì„¹ì…˜ì— í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""),
            ("user", """ì‚¬ìš©ì ì§ˆë¬¸: {question}

ëª¨ë“  ì¸ì‚¬ì´íŠ¸ (ë°˜ë“œì‹œ ëª¨ë‘ í¬í•¨):
{insights}

ë¬¸ì„œ ìš”ì•½ë“¤:
{summaries}

ì¶œì²˜ ë„ë©”ì¸ë“¤: {sources}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.""")
        ])
        
        return await self.llm_service._call_llm(
            prompt_template=prompt_template,
            input_variables={
                "question": title_hint,
                "insights": all_insights_text,
                "summaries": summaries_text,
                "sources": sources_text
            },
            agent_type="response_generator",
            output_parser=self.llm_service.str_parser,
            expected_type=str
        )
    
    def _post_process_response(
        self,
        raw_response: str,
        sources_info: List[Dict[str, str]],
        original_insights: List[str]
    ) -> str:
        """
        ì‘ë‹µ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ ê°œì„ 
        
        Args:
            raw_response: LLMì´ ìƒì„±í•œ ì›ë³¸ ì‘ë‹µ
            sources_info: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            original_insights: ì›ë³¸ ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: í›„ì²˜ë¦¬ëœ ìµœì¢… ì‘ë‹µ
        """
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
            cleaned_response = clean_text(raw_response, remove_extra_whitespace=True)
            
            # 2ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ê²€ì¦ ë° ë³´ì •
            cleaned_response = self._ensure_markdown_structure(cleaned_response)
            
            # 3ë‹¨ê³„: ì¸ì‚¬ì´íŠ¸ ëˆ„ë½ ê²€ì¦ ë° ë³´ì™„
            cleaned_response = self._ensure_all_insights_included(
                cleaned_response, original_insights
            )
            
            # 4ë‹¨ê³„: ì¶œì²˜ ì •ë³´ ì¶”ê°€/ë³´ì™„
            cleaned_response = self._ensure_sources_section(cleaned_response, sources_info)
            
            # 5ë‹¨ê³„: ê¸¸ì´ ì¡°ì •
            if len(cleaned_response) > self.max_response_length:
                cleaned_response = self._truncate_response_smartly(cleaned_response)
            
            return cleaned_response
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return raw_response  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _ensure_markdown_structure(self, response: str) -> str:
        """
        ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ê²€ì¦ ë° ë³´ì •
        
        Args:
            response: ê²€ì¦í•  ì‘ë‹µ
            
        Returns:
            str: êµ¬ì¡°ê°€ ë³´ì •ëœ ì‘ë‹µ
        """
        lines = response.split('\n')
        corrected_lines = []
        
        has_main_title = False
        required_sections = ['ì£¼ìš” í˜„í™©', 'ì£¼ìš” ì¸ì‚¬ì´íŠ¸', 'í–¥í›„ ì „ë§']
        found_sections = set()
        
        for line in lines:
            line = line.strip()
            
            # ë©”ì¸ ì œëª© í™•ì¸
            if line.startswith('# ') and not has_main_title:
                has_main_title = True
                corrected_lines.append(line)
                continue
            
            # ì„¹ì…˜ ì œëª© í™•ì¸
            if line.startswith('## '):
                for section in required_sections:
                    if section in line:
                        found_sections.add(section)
                        break
                corrected_lines.append(line)
                continue
            
            corrected_lines.append(line)
        
        # ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€
        missing_sections = set(required_sections) - found_sections
        if missing_sections:
            logger.warning(f"ëˆ„ë½ëœ ì„¹ì…˜ ê°ì§€: {missing_sections}")
            # ê¸°ë³¸ ì„¹ì…˜ êµ¬ì¡° ì¶”ê°€ (ê°„ë‹¨í•œ í˜•íƒœë¡œ)
            for section in missing_sections:
                corrected_lines.append(f"\n## {section}")
                corrected_lines.append("ì •ë³´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.")
        
        return '\n'.join(corrected_lines)
    
    def _ensure_all_insights_included(self, response: str, original_insights: List[str]) -> str:
        """
        ëª¨ë“  ì¸ì‚¬ì´íŠ¸ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•˜ê³  ëˆ„ë½ëœ ê²ƒ ì¶”ê°€
        
        Args:
            response: ê²€ì¦í•  ì‘ë‹µ
            original_insights: ì›ë³¸ ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: ì¸ì‚¬ì´íŠ¸ê°€ ë³´ì™„ëœ ì‘ë‹µ
        """
        # í˜„ì¬ ì‘ë‹µì—ì„œ ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ ì°¾ê¸°
        insight_pattern = r'## ì£¼ìš” ì¸ì‚¬ì´íŠ¸.*?\n(.*?)(?=\n## |$)'
        match = re.search(insight_pattern, response, re.DOTALL)
        
        if not match:
            logger.warning("ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return response
        
        current_insight_section = match.group(1)
        
        # ëˆ„ë½ëœ ì¸ì‚¬ì´íŠ¸ ì°¾ê¸°
        missing_insights = []
        for insight in original_insights:
            # ì¸ì‚¬ì´íŠ¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            insight_keywords = extract_keywords(insight, max_keywords=3)
            
            # í‚¤ì›Œë“œ ì¤‘ ì¼ë¶€ë¼ë„ í˜„ì¬ ì„¹ì…˜ì— ìˆëŠ”ì§€ í™•ì¸
            if insight_keywords:
                keyword_found = any(
                    keyword.lower() in current_insight_section.lower() 
                    for keyword in insight_keywords
                )
                
                if not keyword_found:
                    missing_insights.append(insight)
        
        # ëˆ„ë½ëœ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
        if missing_insights:
            logger.info(f"ëˆ„ë½ëœ ì¸ì‚¬ì´íŠ¸ {len(missing_insights)}ê°œ ì¶”ê°€")
            
            additional_insights = '\n'.join([f"- {insight}" for insight in missing_insights])
            
            # ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ êµì²´
            new_insight_section = current_insight_section.rstrip() + '\n' + additional_insights
            response = re.sub(
                insight_pattern,
                f'## ì£¼ìš” ì¸ì‚¬ì´íŠ¸ *(ì¶œì²˜: ì°¸ê³  ë¬¸ì„œë“¤)*\n{new_insight_section}',
                response,
                flags=re.DOTALL
            )
        
        return response
    
    def _ensure_sources_section(self, response: str, sources_info: List[Dict[str, str]]) -> str:
        """
        ì¶œì²˜ ì„¹ì…˜ í™•ì¸ ë° ì¶”ê°€
        
        Args:
            response: í™•ì¸í•  ì‘ë‹µ
            sources_info: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: ì¶œì²˜ ì„¹ì…˜ì´ ë³´ì™„ëœ ì‘ë‹µ
        """
        # ì´ë¯¸ ì¶œì²˜ ì„¹ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        if '---' in response and 'ì¶œì²˜' in response:
            return response
        
        # ì¶œì²˜ ì„¹ì…˜ ìƒì„±
        sources_section = "\n\n---\n### ì¶œì²˜\n"
        for source in sources_info[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€
            sources_section += f"- [{source['title']}]({source['url']})\n"
        
        return response + sources_section
    
    def _truncate_response_smartly(self, response: str) -> str:
        """
        ì‘ë‹µì„ ì§€ëŠ¥ì ìœ¼ë¡œ ìë¥´ê¸° (ì„¹ì…˜ êµ¬ì¡° ìœ ì§€)
        
        Args:
            response: ìë¥¼ ì‘ë‹µ
            
        Returns:
            str: ê¸¸ì´ê°€ ì¡°ì •ëœ ì‘ë‹µ
        """
        if len(response) <= self.max_response_length:
            return response
        
        # ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬
        sections = response.split('\n## ')
        
        if len(sections) <= 1:
            # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ìë¥´ê¸°
            return truncate_text(response, self.max_response_length, "\n\n...")
        
        # ê° ì„¹ì…˜ì˜ ëª©í‘œ ê¸¸ì´ ê³„ì‚°
        available_length = self.max_response_length - 200  # ì—¬ìœ ë¶„
        target_lengths = {}
        
        for i, section in enumerate(sections):
            if i == 0:  # ì œëª© ë¶€ë¶„
                target_lengths[i] = min(len(section), 200)
            elif 'ì£¼ìš” í˜„í™©' in section:
                target_lengths[i] = int(available_length * self.section_ratios["í˜„í™©"])
            elif 'ì£¼ìš” ì¸ì‚¬ì´íŠ¸' in section:
                target_lengths[i] = int(available_length * self.section_ratios["ì¸ì‚¬ì´íŠ¸"])
            elif 'í–¥í›„ ì „ë§' in section:
                target_lengths[i] = int(available_length * self.section_ratios["ì „ë§"])
            else:
                target_lengths[i] = min(len(section), 300)
        
        # ê° ì„¹ì…˜ì„ ëª©í‘œ ê¸¸ì´ë¡œ ìë¥´ê¸°
        truncated_sections = []
        for i, section in enumerate(sections):
            if i in target_lengths:
                max_len = target_lengths[i]
                if len(section) > max_len:
                    section = truncate_text(section, max_len, "...")
            
            truncated_sections.append(section)
        
        # ë‹¤ì‹œ ì¡°í•©
        result = truncated_sections[0]  # ì œëª© ë¶€ë¶„
        for section in truncated_sections[1:]:
            result += '\n## ' + section
        
        return result
    
    def _create_empty_response(self) -> ResearchResponse:
        """
        ë¹ˆ ì‘ë‹µ ê°ì²´ ìƒì„± (ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)
        
        Returns:
            ResearchResponse: ê¸°ë³¸ ì‘ë‹µ ê°ì²´
        """
        return ResearchResponse(
            markdown_content="# ì‘ë‹µ ìƒì„± ì‹¤íŒ¨\n\nì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            sources=[],
            insights_count=0,
            documents_used=0,
            word_count=0
        )
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """
        ì²˜ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        return {
            "total_responses_generated": self.total_responses_generated,
            "total_insights_included": self.total_insights_included,
            "total_sources_processed": self.total_sources_processed,
            "max_response_length": self.max_response_length,
            "section_ratios": self.section_ratios,
            "avg_insights_per_response": round(
                self.total_insights_included / max(1, self.total_responses_generated), 2
            )
        }


# LangGraph ë…¸ë“œ í•¨ìˆ˜
async def generate_response_node(state: ResearchState) -> ResearchState:
    """
    ì‘ë‹µ ìƒì„± LangGraph ë…¸ë“œ
    
    Stateì˜ insights, summaries, documentsë¥¼ ë°›ì•„ì„œ ìµœì¢… ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
        
    Returns:
        ResearchState: ìµœì¢… ì‘ë‹µì´ ì¶”ê°€ëœ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    logger.info("=== ì‘ë‹µ ìƒì„± ë…¸ë“œ ì‹œì‘ ===")
    
    try:
        # í˜„ì¬ ë‹¨ê³„ ì„¤ì •
        state = StateManager.set_step(state, "generating_response", "ğŸ“ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
        
        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        insights = state.get("insights", [])
        summaries = state.get("summaries", [])
        documents = state.get("documents", [])
        user_question = state.get("user_input", "")
        
        if not insights:
            error_msg = "ì‘ë‹µ ìƒì„±í•  ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.warning(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # ì‘ë‹µ ìƒì„±ê¸° ìƒì„± ë° ì‹¤í–‰
        generator = ResponseGenerator()
        research_response = await generator.generate_response(
            insights=insights,
            summaries=summaries,
            documents=documents,
            user_question=user_question
        )
        
        if not research_response.markdown_content:
            error_msg = "ë§ˆí¬ë‹¤ìš´ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            logger.error(error_msg)
            return StateManager.set_error(state, error_msg)
        
        # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
        is_valid, validation_errors = validate_response_structure(research_response.markdown_content)
        if not is_valid:
            logger.warning(f"ì‘ë‹µ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {validation_errors}")
            # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ê²€ì¦ ì—ì´ì „íŠ¸ì—ì„œ ë‹¤ì‹œ ì²˜ë¦¬)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        new_state = state.copy()
        new_state["markdown_answer"] = research_response.markdown_content
        
        # ì²˜ë¦¬ í†µê³„ ë¡œê¹…
        stats = generator.get_processing_stats()
        logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: {stats}")
        
        new_state = StateManager.add_log(
            new_state,
            f"ğŸ“ ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {research_response.word_count}ì, "
            f"{research_response.insights_count}ê°œ ì¸ì‚¬ì´íŠ¸, {research_response.documents_used}ê°œ ë¬¸ì„œ í™œìš©"
        )
        
        logger.info("=== ì‘ë‹µ ìƒì„± ë…¸ë“œ ì™„ë£Œ ===")
        return new_state
        
    except Exception as e:
        error_msg = f"ì‘ë‹µ ìƒì„± ë…¸ë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return StateManager.set_error(state, error_msg)


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_response_generator(max_length: int = 8000) -> ResponseGenerator:
    """
    ì‘ë‹µ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        max_length: ìµœëŒ€ ì‘ë‹µ ê¸¸ì´
        
    Returns:
        ResponseGenerator: ì„¤ì •ëœ ì‘ë‹µ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    return ResponseGenerator(max_response_length=max_length)

