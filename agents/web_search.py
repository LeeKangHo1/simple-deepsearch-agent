# agents/web_search.py
"""
ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

ì—¬ëŸ¬ ê²€ìƒ‰ ì—”ì§„ì„ ë³‘ë ¬ë¡œ í™œìš©í•˜ì—¬ í•˜ìœ„ ì¿¼ë¦¬ë“¤ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³ ,
ë²¡í„° DBë¥¼ í†µí•œ ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ì •ë ¬ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- DuckDuckGo + Tavily ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
- ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
- ê¸°ì¡´ ê²°ê³¼ì™€ ìƒˆ ê²°ê³¼ ë³‘í•©
- ë²¡í„° DB ê¸°ë°˜ ì¤‘ë³µ ì œê±° (URL + ë‚´ìš© ìœ ì‚¬ë„)
- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë ¬
- ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§
- ì§„í–‰ ìƒíƒœ ì‹¤ì‹œê°„ ì¶”ì 
"""

import logging
import asyncio
import time
import json
from typing import List, Dict, Any, Optional, Tuple, Set
from collections import defaultdict
import hashlib

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.state import ResearchState, StateManager
from models.data_models import Document, SearchQuery, SearchResult, remove_duplicate_documents
from services.search_service import get_search_service, SearchService
from services.vector_db import get_vector_db_service, VectorDBService
from services.llm_service import get_llm_service
from utils.validators import validate_search_query, validate_document, sanitize_input, validate_query_list
from utils.logger import get_agent_logger, log_agent_start, log_agent_end
from utils.text_processing import extract_keywords, calculate_text_similarity

logger = get_agent_logger("web_search")

class WebSearchAgent:
    """
    ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    
    ì—¬ëŸ¬ ê²€ìƒ‰ ì—”ì§„ì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì´ê³  í¬ê´„ì ì¸ ë¬¸ì„œ ìˆ˜ì§‘ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    ë²¡í„° DBë¥¼ í†µí•œ ì§€ëŠ¥ì  ì¤‘ë³µ ì œê±°ì™€ ê´€ë ¨ì„± ê¸°ë°˜ ì •ë ¬ì„ ì œê³µí•˜ë©°,
    ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 max_results_per_query: int = 3,  # 5 â†’ 3ìœ¼ë¡œ ì¶•ì†Œ
                 max_total_results: int = 10,     # 15 â†’ 10ìœ¼ë¡œ ì¶•ì†Œ
                 similarity_threshold: float = 0.85,
                 enable_content_similarity: bool = False):  # ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        """
        ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            max_results_per_query: ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            max_total_results: ì „ì²´ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ì¤‘ë³µ íŒì • ì„ê³„ê°’ (0.0-1.0)
            enable_content_similarity: ë‚´ìš© ìœ ì‚¬ë„ ê²€ì‚¬ í™œì„±í™”
        """
        self.search_service = get_search_service()
        self.vector_db = get_vector_db_service()
        
        self.max_results_per_query = max_results_per_query
        self.max_total_results = max_total_results
        self.similarity_threshold = similarity_threshold
        self.enable_content_similarity = enable_content_similarity
        
        # ì„±ëŠ¥ í†µê³„
        self.total_searches = 0
        self.total_documents_found = 0
        self.total_documents_after_dedup = 0
        self.avg_search_time = 0.0
        self.search_engine_stats = defaultdict(int)
        self.retry_stats = {"total_retries": 0, "successful_retries": 0}
        
        logger.info(f"ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ "
                   f"(ì¿¼ë¦¬ë‹¹ ìµœëŒ€ {max_results_per_query}ê°œ, "
                   f"ì „ì²´ ìµœëŒ€ {max_total_results}ê°œ, "
                   f"ìœ ì‚¬ë„ ì„ê³„ê°’ {similarity_threshold})")
    
    async def process_state(self, state: ResearchState) -> ResearchState:
        """
        LangGraph ì›Œí¬í”Œë¡œìš°ìš© ìƒíƒœ ì²˜ë¦¬ ë©”ì„œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        
        Args:
            state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
            
        Returns:
            ResearchState: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        """
        log_agent_start("web_search", {"retry_count": state.get("retry_count", 0)})
        
        # ì¬ì‹œë„ ì—¬ë¶€ í™•ì¸
        retry_count = state.get("retry_count", 0)
        is_retry = retry_count > 0
        
        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        if is_retry:
            new_state = StateManager.set_step(
                state, 
                "searching", 
                f"ê²€ì¦ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰ì„ ìˆ˜í–‰ ì¤‘... (ì¬ì‹œë„ {retry_count}íšŒ)"
            )
        else:
            new_state = StateManager.set_step(
                state, 
                "searching", 
                "ì›¹ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ìˆ˜ì§‘ ì¤‘..."
            )
        
        try:
            if is_retry:
                # ì¬ì‹œë„: ìƒˆë¡œìš´ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰
                documents = await self._handle_retry_search(state)
            else:
                # ì´ˆê¸° ê²€ìƒ‰: ê¸°ì¡´ ë¡œì§
                documents = await self._handle_initial_search(state)
            
            # ìƒíƒœì— ê²°ê³¼ ì €ì¥ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ í—ˆìš©)
            new_state = new_state.copy()
            if documents:
                new_state["documents"] = [doc.to_dict() for doc in documents]
            else:
                new_state["documents"] = []
            
            # ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            if is_retry:
                new_state = StateManager.add_log(
                    new_state, 
                    f"âœ… ì¶”ê°€ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(documents)}ê°œ ë¬¸ì„œ (ê¸°ì¡´ + ìƒˆ ê²€ìƒ‰ ê²°ê³¼)"
                )
                self.retry_stats["successful_retries"] += 1
            else:
                new_state = StateManager.add_log(
                    new_state, 
                    f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘"
                )
            
            # ê²€ìƒ‰ ì—”ì§„ë³„ í†µê³„ ë¡œê·¸ ì¶”ê°€
            stats_msg = self._get_search_stats_message()
            new_state = StateManager.add_log(new_state, stats_msg)
            
            log_agent_end("web_search", success=True, 
                         output_data={"document_count": len(documents), "is_retry": is_retry})
            
            logger.info(f"ìƒíƒœ ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œë¥¼ ìƒíƒœì— ì €ì¥ (ì¬ì‹œë„: {is_retry})")
            return new_state
            
        except Exception as e:
            # ì˜¤ë¥˜ ìƒíƒœ ì„¤ì •
            error_msg = f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ ({'ì¬ì‹œë„' if is_retry else 'ì´ˆê¸°ê²€ìƒ‰'}): {e}"
            error_state = StateManager.set_error(new_state, error_msg)
            
            log_agent_end("web_search", success=False, error=str(e))
            logger.error(f"ìƒíƒœ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return error_state
    
    async def _handle_initial_search(self, state: ResearchState) -> List[Document]:
        """
        ì´ˆê¸° ê²€ìƒ‰ ì²˜ë¦¬
        
        Args:
            state: ì—°êµ¬ ìƒíƒœ
            
        Returns:
            List[Document]: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # í•˜ìœ„ ì¿¼ë¦¬ í™•ì¸
        sub_queries = state.get("sub_queries", [])
        if not sub_queries:
            logger.warning("ê²€ìƒ‰í•  í•˜ìœ„ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return []
        
        user_input = state.get("user_input", "")
        
        logger.info(f"ì´ˆê¸° ì›¹ ê²€ìƒ‰ ì‹œì‘: {len(sub_queries)}ê°œ ì¿¼ë¦¬")
        
        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        documents = await self.search_multiple_queries(sub_queries, user_input)
        
        return documents
    
    async def _handle_retry_search(self, state: ResearchState) -> List[Document]:
        """
        ì¬ì‹œë„ ê²€ìƒ‰ ì²˜ë¦¬ (ìƒˆë¡œìš´ ê´€ì ì˜ ì¿¼ë¦¬ + ê¸°ì¡´ ê²°ê³¼ ë³‘í•©)
        
        Args:
            state: ì—°êµ¬ ìƒíƒœ
            
        Returns:
            List[Document]: ê¸°ì¡´ + ìƒˆ ê²€ìƒ‰ ê²°ê³¼ê°€ ë³‘í•©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        self.retry_stats["total_retries"] += 1
        
        # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        user_input = state.get("user_input", "")
        existing_queries = state.get("sub_queries", [])
        validation_feedback = state.get("validation_feedback", "")
        existing_documents_data = state.get("documents", [])
        
        logger.info(f"ì¬ì‹œë„ ê²€ìƒ‰ ì‹œì‘: {len(existing_documents_data)}ê°œ ê¸°ì¡´ ë¬¸ì„œ")
        
        # ê¸°ì¡´ ë¬¸ì„œë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        existing_documents = []
        for doc_data in existing_documents_data:
            try:
                doc = Document(
                    title=doc_data.get("title", ""),
                    url=doc_data.get("url", ""),
                    content=doc_data.get("content", ""),
                    source=doc_data.get("source", "unknown"),
                    relevance_score=doc_data.get("relevance_score", 0.0)
                )
                existing_documents.append(doc)
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        # 1ë‹¨ê³„: LLMìœ¼ë¡œ ìƒˆë¡œìš´ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±
        new_queries = await self._generate_retry_queries(
            user_input, existing_queries, validation_feedback
        )
        
        if not new_queries:
            logger.warning("ìƒˆë¡œìš´ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨, ê¸°ì¡´ ë¬¸ì„œ ë°˜í™˜")
            return existing_documents
        
        logger.info(f"ìƒˆë¡œìš´ ê´€ì ì˜ ì¿¼ë¦¬ {len(new_queries)}ê°œ ìƒì„±: {new_queries}")
        
        # 2ë‹¨ê³„: ìƒˆ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        new_documents = await self.search_multiple_queries(new_queries, user_input)
        
        logger.info(f"ìƒˆ ê²€ìƒ‰ ê²°ê³¼: {len(new_documents)}ê°œ ë¬¸ì„œ")
        
        # 3ë‹¨ê³„: ê¸°ì¡´ + ìƒˆ ë¬¸ì„œ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        all_documents = existing_documents + new_documents
        merged_documents = remove_duplicate_documents(all_documents, threshold=self.similarity_threshold)
        
        logger.info(f"ë¬¸ì„œ ë³‘í•© ì™„ë£Œ: {len(existing_documents)} + {len(new_documents)} â†’ {len(merged_documents)}ê°œ")
        
        # 4ë‹¨ê³„: ìƒíƒœì— ìƒˆ ì¿¼ë¦¬ë„ ì¶”ê°€ (ì¶”ì ìš©)
        # Note: ì´ ë¶€ë¶„ì€ ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë³„ë„ í•„ë“œë¡œ ê´€ë¦¬ í•„ìš”
        
        return merged_documents
    
    async def _generate_retry_queries(self, 
                                    user_question: str, 
                                    existing_queries: List[str], 
                                    validation_feedback: str) -> List[str]:
        """
        ê²€ì¦ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            user_question: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
            existing_queries: ê¸°ì¡´ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤
            validation_feedback: ê²€ì¦ ì—ì´ì „íŠ¸ í”¼ë“œë°±
            
        Returns:
            List[str]: ìƒˆë¡œìš´ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.debug("LLMì„ í†µí•œ ì¬ì‹œë„ ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
            
            llm_service = get_llm_service()
            
            # LLMìœ¼ë¡œ ìƒˆë¡œìš´ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±
            response = await llm_service.generate_retry_queries(
                user_question=user_question,
                existing_queries=existing_queries,
                validation_feedback=validation_feedback
            )
            
            if not response.success:
                logger.error(f"ì¬ì‹œë„ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {response.error_message}")
                return []
            
            # ì‘ë‹µ íŒŒì‹±
            try:
                new_queries = json.loads(response.content)
                if not isinstance(new_queries, list):
                    logger.error("ì¬ì‹œë„ ì¿¼ë¦¬ ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
                    return []
                
                # ì¿¼ë¦¬ ê²€ì¦ ë° ì •ì œ
                validated_queries, removed_count = validate_query_list(new_queries)
                
                if removed_count > 0:
                    logger.debug(f"ì¬ì‹œë„ ì¿¼ë¦¬ ê²€ì¦: {removed_count}ê°œ ì œê±°ë¨")
                
                logger.info(f"ì¬ì‹œë„ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {len(validated_queries)}ê°œ")
                return validated_queries
                
            except json.JSONDecodeError as e:
                logger.error(f"ì¬ì‹œë„ ì¿¼ë¦¬ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return []
                
        except Exception as e:
            logger.error(f"ì¬ì‹œë„ ì¿¼ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def search_multiple_queries(self, 
                                    queries: List[str], 
                                    original_question: str = None) -> List[Document]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•´ ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰ í›„ ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        
        ì´ ë©”ì„œë“œëŠ” ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•œ ê³µê°œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            original_question: ì›ë³¸ ì§ˆë¬¸ (ê´€ë ¨ì„± ì •ë ¬ìš©)
            
        Returns:
            List[Document]: ì¤‘ë³µ ì œê±°ë˜ê³  ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì¿¼ë¦¬ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
            Exception: ê²€ìƒ‰ ì‹¤íŒ¨ ë“± ê¸°íƒ€ ì˜¤ë¥˜
        """
        start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: ì¿¼ë¦¬ ê²€ì¦ ë° ì „ì²˜ë¦¬
            validated_queries = self._validate_and_prepare_queries(queries)
            if not validated_queries:
                raise ValueError("ìœ íš¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: {len(validated_queries)}ê°œ ì¿¼ë¦¬")
            
            # 2ë‹¨ê³„: ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            raw_results = await self._execute_parallel_search(validated_queries)
            logger.debug(f"ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼: {len(raw_results)}ê°œ ë¬¸ì„œ")
            
            # 3ë‹¨ê³„: ë¬¸ì„œ ê²€ì¦ ë° ê¸°ë³¸ í•„í„°ë§
            valid_documents = self._validate_and_filter_documents(raw_results)
            logger.debug(f"ê²€ì¦ëœ ë¬¸ì„œ: {len(valid_documents)}ê°œ")
            
            # 4ë‹¨ê³„: ê°„ë‹¨í•œ ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜ë§Œ)
            deduplicated_docs = self._simple_deduplication(valid_documents)
            logger.debug(f"ì¤‘ë³µ ì œê±° í›„: {len(deduplicated_docs)}ê°œ")
            
            # 5ë‹¨ê³„: ê´€ë ¨ì„± ì •ë ¬ ìƒëµ (ì†ë„ í–¥ìƒ)
            sorted_docs = deduplicated_docs
            
            # 6ë‹¨ê³„: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            final_results = sorted_docs[:self.max_total_results]
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_statistics(len(queries), len(raw_results), len(final_results), processing_time)
            
            logger.info(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ìµœì¢… ë¬¸ì„œ ({processing_time:.2f}ì´ˆ)")
            return final_results
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_statistics(len(queries) if queries else 0, 0, 0, processing_time)
            
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ ({processing_time:.2f}ì´ˆ): {e}")
            raise
    
    def _validate_and_prepare_queries(self, queries: List[str]) -> List[str]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ì„ ê²€ì¦í•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            queries: ì›ë³¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ê²€ì¦ë˜ê³  ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not queries:
            return []
        
        validated_queries = []
        
        for query in queries:
            if not query or not isinstance(query, str):
                logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¿¼ë¦¬ ìŠ¤í‚µ: {query}")
                continue
            
            # ì…ë ¥ ì •ì œ
            cleaned_query = sanitize_input(query.strip(), max_length=200)
            
            # ì¿¼ë¦¬ ê²€ì¦
            is_valid, error_msg = validate_search_query(cleaned_query)
            if is_valid:
                validated_queries.append(cleaned_query)
            else:
                logger.warning(f"ê²€ì¦ ì‹¤íŒ¨ ì¿¼ë¦¬ ìŠ¤í‚µ: '{cleaned_query}' - {error_msg}")
        
        logger.debug(f"ì¿¼ë¦¬ ê²€ì¦ ì™„ë£Œ: {len(queries)}ê°œ â†’ {len(validated_queries)}ê°œ")
        return validated_queries
    
    async def _execute_parallel_search(self, queries: List[str]) -> List[Document]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•´ ë³‘ë ¬ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Document]: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        all_documents = []
        
        # ì¿¼ë¦¬ë³„ ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬ ìƒì„±
        search_tasks = []
        for query in queries:
            task = self._search_single_query(query)
            search_tasks.append(task)
        
        logger.debug(f"ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬ {len(search_tasks)}ê°œ ì‹¤í–‰ ì¤‘...")
        
        # ë³‘ë ¬ ì‹¤í–‰
        try:
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.warning(f"ì¿¼ë¦¬ '{queries[i]}' ê²€ìƒ‰ ì‹¤íŒ¨: {result}")
                    continue
                
                if isinstance(result, list):
                    all_documents.extend(result)
                    logger.debug(f"ì¿¼ë¦¬ '{queries[i]}': {len(result)}ê°œ ë¬¸ì„œ")
        
        except Exception as e:
            logger.error(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê°œë³„ ì¿¼ë¦¬ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ í´ë°±
            all_documents = await self._fallback_sequential_search(queries)
        
        logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘")
        return all_documents
    
    async def _search_single_query(self, query: str) -> List[Document]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•´ ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰í•  ì¿¼ë¦¬
            
        Returns:
            List[Document]: í•´ë‹¹ ì¿¼ë¦¬ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œë“¤
        """
        query_start_time = time.time()
        documents = []
        
        try:
            # SearchServiceë¥¼ í†µí•œ í†µí•© ê²€ìƒ‰ (DuckDuckGo + Tavily)
            search_results = await self.search_service.search_all_engines(
                query=query,
                max_results=self.max_results_per_query
            )
            
            # SearchServiceì—ì„œ ì´ë¯¸ Document ê°ì²´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            for document in search_results:
                try:
                    # ë©”íƒ€ë°ì´í„°ì— ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                    if not hasattr(document, 'metadata') or document.metadata is None:
                        document.metadata = {}
                    
                    document.metadata.update({
                        "query": query,
                        "content_length": len(document.content) if document.content else 0
                    })
                    
                    documents.append(document)
                    
                    # ê²€ìƒ‰ ì—”ì§„ë³„ í†µê³„ ì—…ë°ì´íŠ¸ (sourceëŠ” SearchEngine enum)
                    engine_name = document.source.value if hasattr(document.source, 'value') else str(document.source)
                    self.search_engine_stats[engine_name] += 1
                    
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨ (ì¿¼ë¦¬: {query}): {e}")
                    continue
            
            query_time = time.time() - query_start_time
            logger.debug(f"ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ({query_time:.2f}ì´ˆ)")
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ìœ¼ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨ ë°©ì§€
        
        return documents
    
    async def _fallback_sequential_search(self, queries: List[str]) -> List[Document]:
        """
        ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Document]: ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼
        """
        logger.warning("ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨, ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
        
        all_documents = []
        
        for query in queries:
            try:
                docs = await self._search_single_query(query)
                all_documents.extend(docs)
                
                # ìˆœì°¨ ì‹¤í–‰ì´ë¯€ë¡œ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€ (API ì œí•œ ê³ ë ¤)
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"ìˆœì°¨ ê²€ìƒ‰ì—ì„œ ì¿¼ë¦¬ '{query}' ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: {len(all_documents)}ê°œ ë¬¸ì„œ")
        return all_documents
    
    def _validate_and_filter_documents(self, documents: List[Document]) -> List[Document]:
        """
        ìˆ˜ì§‘ëœ ë¬¸ì„œë“¤ì„ ê²€ì¦í•˜ê³  ê¸°ë³¸ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Document]: ê²€ì¦ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents:
            return []
        
        valid_documents = []
        filtered_count = 0
        
        seen_urls = set()  # URL ê¸°ë°˜ ê¸°ë³¸ ì¤‘ë³µ ì œê±°
        
        for doc in documents:
            try:
                # ë¬¸ì„œ ìœ íš¨ì„± ê²€ì¦
                is_valid, errors = validate_document(doc)
                if not is_valid:
                    filtered_count += 1
                    logger.debug(f"ë¬¸ì„œ ê²€ì¦ ì‹¤íŒ¨: {errors}")
                    continue
                
                # URL ê¸°ë°˜ ê¸°ë³¸ ì¤‘ë³µ ì œê±°
                if doc.url in seen_urls:
                    filtered_count += 1
                    logger.debug(f"URL ì¤‘ë³µ ì œê±°: {doc.url}")
                    continue
                
                # ë‚´ìš© ê¸¸ì´ ê¸°ë³¸ ê²€ì¦
                if not doc.content or len(doc.content.strip()) < 50:
                    filtered_count += 1
                    logger.debug(f"ë‚´ìš© ë¶€ì¡±ìœ¼ë¡œ ì œê±°: {doc.title}")
                    continue
                
                # ë‚´ìš© í’ˆì§ˆ ê¸°ë³¸ ê²€ì¦
                if self._is_low_quality_content(doc.content):
                    filtered_count += 1
                    logger.debug(f"ì €í’ˆì§ˆ ë‚´ìš©ìœ¼ë¡œ ì œê±°: {doc.title}")
                    continue
                
                seen_urls.add(doc.url)
                valid_documents.append(doc)
                
            except Exception as e:
                logger.warning(f"ë¬¸ì„œ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                filtered_count += 1
                continue
        
        logger.debug(f"ë¬¸ì„œ ê²€ì¦ ì™„ë£Œ: {len(documents)}ê°œ â†’ {len(valid_documents)}ê°œ "
                    f"(í•„í„°ë§ë¨: {filtered_count}ê°œ)")
        
        return valid_documents
    
    def _simple_deduplication(self, documents: List[Document]) -> List[Document]:
        """
        ê°„ë‹¨í•œ URL ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì†ë„ ìµœì í™”)
        
        Args:
            documents: ì¤‘ë³µ ì œê±°í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Document]: ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        seen_urls = set()
        unique_docs = []
        
        for doc in documents:
            if doc.url not in seen_urls:
                seen_urls.add(doc.url)
                unique_docs.append(doc)
        
        return unique_docs
    
    def _is_low_quality_content(self, content: str) -> bool:
        """
        ì €í’ˆì§ˆ ë‚´ìš©ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        
        Args:
            content: ê²€ì‚¬í•  ë‚´ìš©
            
        Returns:
            bool: ì €í’ˆì§ˆ ë‚´ìš© ì—¬ë¶€
        """
        if not content:
            return True
        
        content_lower = content.lower()
        
        # ì €í’ˆì§ˆ íŒ¨í„´ë“¤
        low_quality_patterns = [
            "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†",
            "access denied",
            "404 not found",
            "error occurred",
            "javascriptë¥¼ í™œì„±í™”",
            "enable javascript",
            "ì¿ í‚¤ë¥¼ í—ˆìš©",
            "allow cookies",
            "ë¡œê·¸ì¸ì´ í•„ìš”",
            "login required",
            "êµ¬ë…ì´ í•„ìš”",
            "subscription required"
        ]
        
        for pattern in low_quality_patterns:
            if pattern in content_lower:
                return True
        
        # ì¤‘ë³µ ë¬¸ì/ë‹¨ì–´ ë¹„ìœ¨ ê²€ì‚¬
        words = content.split()
        if len(words) > 10:
            unique_words = set(words)
            if len(unique_words) / len(words) < 0.3:  # ì¤‘ë³µ ë¹„ìœ¨ 70% ì´ìƒ
                return True
        
        return False
    
    async def _remove_duplicates_with_vector_db(self, documents: List[Document]) -> List[Document]:
        """
        ë²¡í„° DBë¥¼ í™œìš©í•œ ê³ ê¸‰ ì¤‘ë³µ ì œê±°
        
        URL ì¤‘ë³µ ì œê±°ëŠ” ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœì—ì„œ ë‚´ìš© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ìˆ˜í–‰
        
        Args:
            documents: ì¤‘ë³µ ì œê±°í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Document]: ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents or not self.enable_content_similarity:
            return documents
        
        if len(documents) <= 1:
            return documents
        
        try:
            logger.debug(f"ë²¡í„° DB ì¤‘ë³µ ì œê±° ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
            
            # ë‹¨ìˆœí•œ ì¤‘ë³µ ì œê±° ì‚¬ìš© (ë²¡í„° DB ì˜ì¡´ì„± ì¤„ì´ê¸°)
            deduplicated_docs = remove_duplicate_documents(documents, threshold=self.similarity_threshold)
            
            removed_count = len(documents) - len(deduplicated_docs)
            if removed_count > 0:
                logger.info(f"ë‚´ìš© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: {removed_count}ê°œ ë¬¸ì„œ ì œê±°")
            
            return deduplicated_docs
            
        except Exception as e:
            logger.warning(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            return documents
    
    async def _sort_by_relevance(self, documents: List[Document], original_question: str) -> List[Document]:
        """
        ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì •ë ¬í•©ë‹ˆë‹¤.
        
        Args:
            documents: ì •ë ¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            original_question: ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            List[Document]: ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents or not original_question:
            return documents
        
        try:
            logger.debug(f"ê´€ë ¨ì„± ê¸°ë°˜ ì •ë ¬ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ê´€ë ¨ì„± ê³„ì‚°
            question_keywords = set(extract_keywords(original_question, max_keywords=10))
            
            for doc in documents:
                # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                title_keywords = set(extract_keywords(doc.title, max_keywords=5))
                content_keywords = set(extract_keywords(doc.content[:500], max_keywords=10))  # ì²« 500ìë§Œ
                doc_keywords = title_keywords | content_keywords
                
                # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ê³„ì‚° (ì œëª© ê°€ì¤‘ì¹˜ ë†’ê²Œ)
                title_overlap = len(question_keywords & title_keywords)
                content_overlap = len(question_keywords & content_keywords)
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
                title_score = title_overlap / max(len(question_keywords), 1) * 0.7
                content_score = content_overlap / max(len(question_keywords), 1) * 0.3
                doc.relevance_score = title_score + content_score
            
            # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_docs = sorted(documents, key=lambda x: x.relevance_score, reverse=True)
            
            logger.debug(f"ê´€ë ¨ì„± ì •ë ¬ ì™„ë£Œ: ìƒìœ„ ë¬¸ì„œ ì ìˆ˜ {sorted_docs[0].relevance_score:.3f}")
            return sorted_docs
            
        except Exception as e:
            logger.warning(f"ê´€ë ¨ì„± ì •ë ¬ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {e}")
            return documents
    
    def _update_statistics(self, query_count: int, raw_docs: int, final_docs: int, processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_searches += 1
        self.total_documents_found += raw_docs
        self.total_documents_after_dedup += final_docs
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        if self.total_searches == 1:
            self.avg_search_time = processing_time
        else:
            self.avg_search_time = (
                (self.avg_search_time * (self.total_searches - 1) + processing_time) 
                / self.total_searches
            )
    
    def _get_search_stats_message(self) -> str:
        """ê²€ìƒ‰ í†µê³„ ë©”ì‹œì§€ ìƒì„±"""
        engine_stats = []
        for engine, count in self.search_engine_stats.items():
            engine_stats.append(f"{engine}: {count}ê°œ")
        
        retry_info = f"ì¬ì‹œë„: {self.retry_stats['total_retries']}íšŒ"
        
        return f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ - {', '.join(engine_stats)}, {retry_info}"
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì„±ëŠ¥ í†µê³„ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì„±ëŠ¥ í†µê³„ ì •ë³´
        """
        return {
            "total_searches": self.total_searches,
            "total_documents_found": self.total_documents_found,
            "total_documents_after_dedup": self.total_documents_after_dedup,
            "deduplication_rate": (
                (self.total_documents_found - self.total_documents_after_dedup) 
                / max(self.total_documents_found, 1) * 100
            ),
            "avg_documents_per_search": (
                self.total_documents_after_dedup / max(self.total_searches, 1)
            ),
            "avg_search_time": round(self.avg_search_time, 3),
            "search_engine_stats": dict(self.search_engine_stats),
            "retry_stats": dict(self.retry_stats),
            "max_results_per_query": self.max_results_per_query,
            "max_total_results": self.max_total_results,
            "similarity_threshold": self.similarity_threshold,
            "content_similarity_enabled": self.enable_content_similarity
        }
    
    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.total_searches = 0
        self.total_documents_found = 0
        self.total_documents_after_dedup = 0
        self.avg_search_time = 0.0
        self.search_engine_stats.clear()
        self.retry_stats = {"total_retries": 0, "successful_retries": 0}
        logger.info("ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ í†µê³„ ì´ˆê¸°í™”ë¨")


# LangGraph ë…¸ë“œ í•¨ìˆ˜
async def web_search_node(state: ResearchState) -> ResearchState:
    """
    LangGraph ì›Œí¬í”Œë¡œìš°ìš© ì›¹ ê²€ìƒ‰ ë…¸ë“œ í•¨ìˆ˜
    
    Args:
        state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
        
    Returns:
        ResearchState: ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
    """
    web_search_agent = get_web_search_agent()
    return await web_search_agent.process_state(state)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_web_search_agent_instance = None

def get_web_search_agent() -> WebSearchAgent:
    """
    ì „ì—­ ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        WebSearchAgent: ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    """
    global _web_search_agent_instance
    
    if _web_search_agent_instance is None:
        _web_search_agent_instance = WebSearchAgent()
        logger.info("ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨")
    
    return _web_search_agent_instance

def reset_web_search_agent():
    """ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ìš©)"""
    global _web_search_agent_instance
    _web_search_agent_instance = None
    logger.info("ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹ë¨")